{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The hybrid model performes better than the stand-alone transformer model, the training start with 12 piano scripts and received the result as 0.63 as accuracy. But with the increase of the samples to 30 scripts, the accuracy increase to 0.67"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Input Layer:\n",
    "\n",
    "Input dimension: 10 (number of features in the input)\n",
    "Positional Encoding:\n",
    "\n",
    "Model dimension: 64\n",
    "Max length: 5000\n",
    "Transformer Encoder Layer (6 layers):\n",
    "\n",
    "Model dimension: 64\n",
    "Number of heads: 4\n",
    "Feedforward dimension: 256\n",
    "Dropout: 0.1\n",
    "Activation: ReLU\n",
    "RNN Layer:\n",
    "\n",
    "RNN type: LSTM\n",
    "Hidden dimension: 128\n",
    "Number of layers: 1\n",
    "Dropout: 0.1\n",
    "Classifier:\n",
    "\n",
    "Flatten\n",
    "Linear layer: 128 * sequence_length to 256\n",
    "Activation: ReLU\n",
    "Dropout: 0.1\n",
    "Linear layer: 256 to n_classes (8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting epoch 1/50\n",
      "Epoch 1/50, Loss: 1.0982, Accuracy: 0.5564\n",
      "Evaluation Loss: 1.0187, Accuracy: 0.5912\n",
      "Starting epoch 2/50\n",
      "Epoch 2/50, Loss: 0.9749, Accuracy: 0.5998\n",
      "Evaluation Loss: 0.9257, Accuracy: 0.6164\n",
      "Starting epoch 3/50\n",
      "Epoch 3/50, Loss: 0.9152, Accuracy: 0.6167\n",
      "Evaluation Loss: 0.8893, Accuracy: 0.6281\n",
      "Starting epoch 4/50\n",
      "Epoch 4/50, Loss: 0.8897, Accuracy: 0.6267\n",
      "Evaluation Loss: 0.8749, Accuracy: 0.6334\n",
      "Starting epoch 5/50\n",
      "Epoch 5/50, Loss: 0.8732, Accuracy: 0.6339\n",
      "Evaluation Loss: 0.8627, Accuracy: 0.6400\n",
      "Starting epoch 6/50\n",
      "Epoch 6/50, Loss: 0.8617, Accuracy: 0.6390\n",
      "Evaluation Loss: 0.8558, Accuracy: 0.6438\n",
      "Starting epoch 7/50\n",
      "Epoch 7/50, Loss: 0.8524, Accuracy: 0.6433\n",
      "Evaluation Loss: 0.8465, Accuracy: 0.6461\n",
      "Starting epoch 8/50\n",
      "Epoch 8/50, Loss: 0.8448, Accuracy: 0.6468\n",
      "Evaluation Loss: 0.8442, Accuracy: 0.6485\n",
      "Starting epoch 9/50\n",
      "Epoch 9/50, Loss: 0.8389, Accuracy: 0.6491\n",
      "Evaluation Loss: 0.8363, Accuracy: 0.6522\n",
      "Starting epoch 10/50\n",
      "Epoch 10/50, Loss: 0.8335, Accuracy: 0.6515\n",
      "Evaluation Loss: 0.8303, Accuracy: 0.6542\n",
      "Starting epoch 11/50\n",
      "Epoch 11/50, Loss: 0.8286, Accuracy: 0.6538\n",
      "Evaluation Loss: 0.8270, Accuracy: 0.6569\n",
      "Starting epoch 12/50\n",
      "Epoch 12/50, Loss: 0.8252, Accuracy: 0.6557\n",
      "Evaluation Loss: 0.8291, Accuracy: 0.6529\n",
      "Starting epoch 13/50\n",
      "Epoch 13/50, Loss: 0.8211, Accuracy: 0.6579\n",
      "Evaluation Loss: 0.8226, Accuracy: 0.6570\n",
      "Starting epoch 14/50\n",
      "Epoch 14/50, Loss: 0.8176, Accuracy: 0.6595\n",
      "Evaluation Loss: 0.8202, Accuracy: 0.6587\n",
      "Starting epoch 15/50\n",
      "Epoch 15/50, Loss: 0.8157, Accuracy: 0.6601\n",
      "Evaluation Loss: 0.8187, Accuracy: 0.6601\n",
      "Starting epoch 16/50\n",
      "Epoch 16/50, Loss: 0.8117, Accuracy: 0.6624\n",
      "Evaluation Loss: 0.8184, Accuracy: 0.6587\n",
      "Starting epoch 17/50\n",
      "Epoch 17/50, Loss: 0.8094, Accuracy: 0.6633\n",
      "Evaluation Loss: 0.8131, Accuracy: 0.6607\n",
      "Starting epoch 18/50\n",
      "Epoch 18/50, Loss: 0.8071, Accuracy: 0.6642\n",
      "Evaluation Loss: 0.8120, Accuracy: 0.6633\n",
      "Starting epoch 19/50\n",
      "Epoch 19/50, Loss: 0.8040, Accuracy: 0.6655\n",
      "Evaluation Loss: 0.8114, Accuracy: 0.6619\n",
      "Starting epoch 20/50\n",
      "Epoch 20/50, Loss: 0.8016, Accuracy: 0.6665\n",
      "Evaluation Loss: 0.8087, Accuracy: 0.6647\n",
      "Starting epoch 21/50\n",
      "Epoch 21/50, Loss: 0.7995, Accuracy: 0.6679\n",
      "Evaluation Loss: 0.8052, Accuracy: 0.6649\n",
      "Starting epoch 22/50\n",
      "Epoch 22/50, Loss: 0.7982, Accuracy: 0.6681\n",
      "Evaluation Loss: 0.8063, Accuracy: 0.6660\n",
      "Starting epoch 23/50\n",
      "Epoch 23/50, Loss: 0.7973, Accuracy: 0.6690\n",
      "Evaluation Loss: 0.8047, Accuracy: 0.6669\n",
      "Starting epoch 24/50\n",
      "Epoch 24/50, Loss: 0.7953, Accuracy: 0.6696\n",
      "Evaluation Loss: 0.8046, Accuracy: 0.6663\n",
      "Starting epoch 25/50\n",
      "Epoch 25/50, Loss: 0.7930, Accuracy: 0.6707\n",
      "Evaluation Loss: 0.8054, Accuracy: 0.6660\n",
      "Starting epoch 26/50\n",
      "Epoch 26/50, Loss: 0.7915, Accuracy: 0.6707\n",
      "Evaluation Loss: 0.8022, Accuracy: 0.6668\n",
      "Starting epoch 27/50\n",
      "Epoch 27/50, Loss: 0.7894, Accuracy: 0.6720\n",
      "Evaluation Loss: 0.7996, Accuracy: 0.6689\n",
      "Starting epoch 28/50\n",
      "Epoch 28/50, Loss: 0.7875, Accuracy: 0.6734\n",
      "Evaluation Loss: 0.8013, Accuracy: 0.6673\n",
      "Starting epoch 29/50\n",
      "Epoch 29/50, Loss: 0.7862, Accuracy: 0.6746\n",
      "Evaluation Loss: 0.7992, Accuracy: 0.6693\n",
      "Starting epoch 30/50\n",
      "Epoch 30/50, Loss: 0.7851, Accuracy: 0.6740\n",
      "Evaluation Loss: 0.7974, Accuracy: 0.6695\n",
      "Starting epoch 31/50\n",
      "Epoch 31/50, Loss: 0.7837, Accuracy: 0.6750\n",
      "Evaluation Loss: 0.7975, Accuracy: 0.6695\n",
      "Starting epoch 32/50\n",
      "Epoch 32/50, Loss: 0.7827, Accuracy: 0.6761\n",
      "Evaluation Loss: 0.7935, Accuracy: 0.6716\n",
      "Starting epoch 33/50\n",
      "Epoch 33/50, Loss: 0.7812, Accuracy: 0.6762\n",
      "Evaluation Loss: 0.7976, Accuracy: 0.6692\n",
      "Starting epoch 34/50\n",
      "Epoch 34/50, Loss: 0.7800, Accuracy: 0.6764\n",
      "Evaluation Loss: 0.7930, Accuracy: 0.6716\n",
      "Starting epoch 35/50\n",
      "Epoch 35/50, Loss: 0.7783, Accuracy: 0.6779\n",
      "Evaluation Loss: 0.7936, Accuracy: 0.6714\n",
      "Starting epoch 36/50\n",
      "Epoch 36/50, Loss: 0.7777, Accuracy: 0.6777\n",
      "Evaluation Loss: 0.7918, Accuracy: 0.6719\n",
      "Starting epoch 37/50\n",
      "Epoch 37/50, Loss: 0.7768, Accuracy: 0.6782\n",
      "Evaluation Loss: 0.7917, Accuracy: 0.6732\n",
      "Starting epoch 38/50\n",
      "Epoch 38/50, Loss: 0.7757, Accuracy: 0.6788\n",
      "Evaluation Loss: 0.7904, Accuracy: 0.6733\n",
      "Starting epoch 39/50\n",
      "Epoch 39/50, Loss: 0.7747, Accuracy: 0.6796\n",
      "Evaluation Loss: 0.7904, Accuracy: 0.6731\n",
      "Starting epoch 40/50\n",
      "Epoch 40/50, Loss: 0.7738, Accuracy: 0.6793\n",
      "Evaluation Loss: 0.7895, Accuracy: 0.6730\n",
      "Starting epoch 41/50\n",
      "Epoch 41/50, Loss: 0.7734, Accuracy: 0.6800\n",
      "Evaluation Loss: 0.7903, Accuracy: 0.6725\n",
      "Starting epoch 42/50\n",
      "Epoch 42/50, Loss: 0.7720, Accuracy: 0.6808\n",
      "Evaluation Loss: 0.7893, Accuracy: 0.6734\n",
      "Starting epoch 43/50\n",
      "Epoch 43/50, Loss: 0.7711, Accuracy: 0.6809\n",
      "Evaluation Loss: 0.7883, Accuracy: 0.6732\n",
      "Starting epoch 44/50\n",
      "Epoch 44/50, Loss: 0.7699, Accuracy: 0.6813\n",
      "Evaluation Loss: 0.7880, Accuracy: 0.6733\n",
      "Starting epoch 45/50\n",
      "Epoch 45/50, Loss: 0.7691, Accuracy: 0.6823\n",
      "Evaluation Loss: 0.7887, Accuracy: 0.6735\n",
      "Starting epoch 46/50\n",
      "Epoch 46/50, Loss: 0.7683, Accuracy: 0.6823\n",
      "Evaluation Loss: 0.7863, Accuracy: 0.6736\n",
      "Starting epoch 47/50\n",
      "Epoch 47/50, Loss: 0.7677, Accuracy: 0.6829\n",
      "Evaluation Loss: 0.7879, Accuracy: 0.6733\n",
      "Starting epoch 48/50\n",
      "Epoch 48/50, Loss: 0.7665, Accuracy: 0.6832\n",
      "Evaluation Loss: 0.7872, Accuracy: 0.6730\n",
      "Starting epoch 49/50\n",
      "Epoch 49/50, Loss: 0.7655, Accuracy: 0.6834\n",
      "Evaluation Loss: 0.7871, Accuracy: 0.6738\n",
      "Starting epoch 50/50\n",
      "Epoch 50/50, Loss: 0.7642, Accuracy: 0.6844\n",
      "Evaluation Loss: 0.7839, Accuracy: 0.6751\n",
      "Evaluation Loss: 0.7812, Accuracy: 0.6760\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.781249212879889, 0.6759686576579568)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import torch.optim as optim\n",
    "import random\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, model_dim, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        pe = torch.zeros(max_len, model_dim)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, model_dim, 2).float() * (-np.log(10000.0) / model_dim))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:, :x.size(1)]\n",
    "        return x\n",
    "\n",
    "class CustomTransformerEncoderLayer(nn.Module):\n",
    "    def __init__(self, model_dim, num_heads, ff_dim, dropout=0.1, activation='relu'):\n",
    "        super(CustomTransformerEncoderLayer, self).__init__()\n",
    "        self.self_attn = nn.MultiheadAttention(model_dim, num_heads, dropout=dropout, batch_first=True)\n",
    "        self.linear1 = nn.Linear(model_dim, ff_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.linear2 = nn.Linear(ff_dim, model_dim)\n",
    "        \n",
    "        # Choose activation function\n",
    "        if activation == 'relu':\n",
    "            self.activation = nn.ReLU()\n",
    "        elif activation == 'gelu':\n",
    "            self.activation = nn.GELU()\n",
    "        elif activation == 'softmax':\n",
    "            self.activation = nn.Softmax(dim=-1)\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported activation function: {activation}\")\n",
    "        \n",
    "        self.norm1 = nn.LayerNorm(model_dim)\n",
    "        self.norm2 = nn.LayerNorm(model_dim)\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, src):\n",
    "        src2 = self.self_attn(src, src, src)[0]\n",
    "        src = src + self.dropout1(src2)\n",
    "        src = self.norm1(src)\n",
    "        src2 = self.linear2(self.dropout(self.activation(self.linear1(src))))\n",
    "        src = src + self.dropout2(src2)\n",
    "        src = self.norm2(src)\n",
    "        return src\n",
    "\n",
    "class TransformerRNNHybridModel(nn.Module):\n",
    "    def __init__(self, input_dim, model_dim, num_heads, num_layers, ff_dim, rnn_hidden_dim, n_classes, dropout=0.1, activation='relu'):\n",
    "        super(TransformerRNNHybridModel, self).__init__()\n",
    "        self.model_dim = model_dim\n",
    "        self.input_projection = nn.Linear(input_dim, model_dim)  \n",
    "        self.pos_encoder = PositionalEncoding(model_dim)\n",
    "        \n",
    "        # Create custom transformer encoder layers\n",
    "        self.encoder_layers = nn.ModuleList([\n",
    "            CustomTransformerEncoderLayer(model_dim, num_heads, ff_dim, dropout, activation)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "        \n",
    "        self.rnn = nn.LSTM(model_dim, rnn_hidden_dim, batch_first=True)\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(rnn_hidden_dim * sequence_length, n_classes)\n",
    "        )\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        initrange = 0.1\n",
    "        for layer in self.classifier:\n",
    "            if isinstance(layer, nn.Linear):\n",
    "                layer.bias.data.zero_()\n",
    "                layer.weight.data.uniform_(-initrange, initrange)\n",
    "\n",
    "    def forward(self, src):\n",
    "        src = self.input_projection(src) * np.sqrt(self.model_dim)  \n",
    "        src = self.pos_encoder(src)\n",
    "        for layer in self.encoder_layers:\n",
    "            src = layer(src)\n",
    "        rnn_output, _ = self.rnn(src)\n",
    "        output = self.classifier(rnn_output)\n",
    "        return output\n",
    "\n",
    "class PianoDataset(Dataset):\n",
    "    def __init__(self, csv_file, sequence_length=100, augment=False):\n",
    "        self.df = pd.read_csv(csv_file)\n",
    "        self.df = pd.concat([self.df.drop('event_type', axis=1), pd.get_dummies(self.df['event_type'], prefix='event_type')], axis=1)\n",
    "        self.df = self.df.astype(np.float32)  \n",
    "        self.sequence_length = sequence_length\n",
    "        self.augment = augment\n",
    "        self.data = self.generate_sequences()\n",
    "\n",
    "    def generate_sequences(self):\n",
    "        input_sequences = []\n",
    "        target_events = []\n",
    "        for i in range(len(self.df) - self.sequence_length):\n",
    "            full_sequence = self.df.iloc[i:i+self.sequence_length+1]\n",
    "            input_sequence = full_sequence.iloc[:-1]\n",
    "            target_event = full_sequence.iloc[-1]\n",
    "            input_sequences.append(input_sequence.values)\n",
    "            target_events.append(target_event.values.argmax())  \n",
    "        return list(zip(input_sequences, target_events))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sequence, target = self.data[idx]\n",
    "        if self.augment:\n",
    "            sequence = self.augment_sequence(sequence)\n",
    "        sequence_tensor = torch.tensor(sequence, dtype=torch.float32)\n",
    "        target_tensor = torch.tensor(target, dtype=torch.long)\n",
    "        return sequence_tensor, target_tensor\n",
    "\n",
    "    def augment_sequence(self, sequence):\n",
    "        # Transposition\n",
    "        if random.random() < 0.5:\n",
    "            transpose_amount = random.randint(-5, 5)\n",
    "            sequence[:, 0] += transpose_amount  \n",
    "        \n",
    "        # Time-stretching\n",
    "        if random.random() < 0.5:\n",
    "            stretch_factor = random.uniform(0.8, 1.2)\n",
    "            sequence[:, 1] *= stretch_factor  \n",
    "        \n",
    "        # Adding Noise\n",
    "        if random.random() < 0.5:\n",
    "            noise = np.random.normal(0, 0.01, sequence.shape)\n",
    "            sequence += noise\n",
    "        \n",
    "        return sequence\n",
    "\n",
    "def train_valid_test_split(dataset, train_ratio=0.7, valid_ratio=0.15, test_ratio=0.15):\n",
    "    assert train_ratio + valid_ratio + test_ratio == 1, \"Ratios must sum to 1\"\n",
    "    \n",
    "    train_size = int(train_ratio * len(dataset))\n",
    "    valid_size = int(valid_ratio * len(dataset))\n",
    "    test_size = len(dataset) - train_size - valid_size\n",
    "    \n",
    "    train_dataset, valid_dataset, test_dataset = random_split(dataset, [train_size, valid_size, test_size])\n",
    "    return train_dataset, valid_dataset, test_dataset\n",
    "\n",
    "# Define model parameters\n",
    "input_dim = 10 \n",
    "model_dim = 64  \n",
    "num_heads = 4 \n",
    "num_layers = 4  \n",
    "ff_dim = 128  \n",
    "rnn_hidden_dim = 32  \n",
    "n_classes = 8 \n",
    "sequence_length = 100\n",
    "\n",
    "# Set device and initialize model and optimizer\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "activation_function = 'relu'  # Change activation function here\n",
    "model = TransformerRNNHybridModel(input_dim, model_dim, num_heads, num_layers, ff_dim, rnn_hidden_dim, n_classes, activation=activation_function).to(device)\n",
    "\n",
    "# Initialize weights\n",
    "def initialize_weights(m):\n",
    "    if isinstance(m, nn.Linear):\n",
    "        torch.nn.init.xavier_uniform_(m.weight)\n",
    "        m.bias.data.fill_(0.01)\n",
    "\n",
    "model.apply(initialize_weights)\n",
    "\n",
    "# Set optimizer with a lower learning rate\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
    "\n",
    "# Define loss function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Create dataset and dataloaders\n",
    "output_csv = 'D:\\\\Projects\\\\Resources\\\\midis_v1.2\\\\Beethoven\\\\Beethoven_note_sequence.csv'\n",
    "full_dataset = PianoDataset(output_csv, sequence_length, augment=True)\n",
    "\n",
    "# Split the dataset\n",
    "train_dataset, valid_dataset, test_dataset = train_valid_test_split(full_dataset)\n",
    "\n",
    "# Create DataLoaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)  # Reduced batch size\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=128, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# Training function with gradient clipping and early stopping\n",
    "def train_model(model, train_loader, valid_loader, optimizer, num_epochs=50, patience=5): \n",
    "    model.train()\n",
    "    best_accuracy = 0.0\n",
    "    patience_counter = 0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss = 0.0\n",
    "        correct_predictions = 0\n",
    "        total_predictions = 0\n",
    "        print(f'Starting epoch {epoch+1}/{num_epochs}')\n",
    "        \n",
    "        for batch_idx, (inputs, targets) in enumerate(train_loader):\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            \n",
    "            # Ensure outputs and targets are correctly shaped\n",
    "            if outputs.shape[0] != targets.shape[0]:\n",
    "                print(f\"Shape mismatch: outputs shape {outputs.shape}, targets shape {targets.shape}\")\n",
    "                continue\n",
    "            \n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item() * inputs.size(0)  \n",
    "            \n",
    "            # Calculate accuracy\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            correct_predictions += (predicted == targets).sum().item()\n",
    "            total_predictions += targets.size(0)\n",
    "\n",
    "        epoch_loss = running_loss / len(train_loader.dataset)\n",
    "        epoch_accuracy = correct_predictions / total_predictions\n",
    "        print(f'Epoch {epoch+1}/{num_epochs}, Loss: {epoch_loss:.4f}, Accuracy: {epoch_accuracy:.4f}')\n",
    "        \n",
    "        # Validate the model\n",
    "        val_loss, val_accuracy = evaluate_model(model, valid_loader)\n",
    "        \n",
    "        # Early stopping\n",
    "        if val_accuracy > best_accuracy:\n",
    "            best_accuracy = val_accuracy\n",
    "            patience_counter = 0\n",
    "            torch.save(model.state_dict(), 'best_model_hybrid.pth')\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= patience:\n",
    "                print(f'Early stopping at epoch {epoch+1}')\n",
    "                break\n",
    "\n",
    "def evaluate_model(model, dataloader):\n",
    "    model.eval()  \n",
    "    running_loss = 0.0\n",
    "    correct_predictions = 0\n",
    "    total_predictions = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in dataloader:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            running_loss += loss.item() * inputs.size(0)  \n",
    "            \n",
    "            # Calculate accuracy\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            correct_predictions += (predicted == targets).sum().item()\n",
    "            total_predictions += targets.size(0)\n",
    "\n",
    "    total_loss = running_loss / len(dataloader.dataset)\n",
    "    accuracy = correct_predictions / total_predictions\n",
    "    print(f'Evaluation Loss: {total_loss:.4f}, Accuracy: {accuracy:.4f}')\n",
    "    model.train() \n",
    "    return total_loss, accuracy\n",
    "\n",
    "# Train the model with high patience early stopping and save the best model\n",
    "train_model(model, train_loader, valid_loader, optimizer, num_epochs=50, patience=5)\n",
    "\n",
    "# Load the best model and evaluate on the test set\n",
    "model.load_state_dict(torch.load('best_model_hybrid.pth'))\n",
    "evaluate_model(model, test_loader)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can slightly change the model structure and see the performance without changing the input data to see the potential of the hybrid model.\n",
    "\n",
    "RNN Configuration:\n",
    "Original Model: Uses a unidirectional LSTM with a single layer (bidirectional not specified and num_layers not set to 2).\n",
    "Tuned Model: Uses a bidirectional LSTM with 2 layers (bidirectional=True and num_layers=2).\n",
    "\n",
    "\n",
    "Attention Mechanism:\n",
    "Original Model: Does not include an attention mechanism.\n",
    "Tuned Model: Includes a multihead attention mechanism applied to the output of the LSTM (self.attention = nn.MultiheadAttention(rnn_hidden_dim * 2, num_heads, dropout=dropout, batch_first=True)).\n",
    "\n",
    "Convolutional Layers:\n",
    "Original Model: Does not include convolutional layers.\n",
    "Tuned Model: Includes convolutional layers after the attention mechanism (self.conv1, self.conv2, and self.pool).\n",
    "\n",
    "Classifier Input:\n",
    "Original Model: The classifier input is based directly on the output of the LSTM (nn.Linear(rnn_hidden_dim * sequence_length, n_classes)).\n",
    "Tuned Model: The classifier input is adjusted based on the output of the convolutional layers (nn.Linear(128 * 25, 512)).\n",
    "\n",
    "Model Dimensionality:\n",
    "Original Model: Uses a smaller model dimension (model_dim = 64), fewer transformer layers (num_layers = 4), and a smaller feedforward dimension (ff_dim = 128).\n",
    "Tuned Model: Uses a larger model dimension (model_dim = 128), more transformer layers (num_layers = 8), and a larger feedforward dimension (ff_dim = 512).\n",
    "\n",
    "Batch Size and Training Configuration:\n",
    "Original Model: Uses a batch size of 128 for training and validation.\n",
    "Tuned Model: Uses a batch size of 64 for training and validation.\n",
    "\n",
    "Optimization and Training Parameters:\n",
    "Original Model: Does not use a learning rate scheduler and has a reduced patience of 5 for early stopping.\n",
    "Tuned Model: Uses a learning rate scheduler (scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)) and a patience of 10 for early stopping.\n",
    "\n",
    "See from the result we can see that this is the only model with the same input data achieved 0.70 accuracy. It shows the transformer_RNN hybrid model have the best potencial in achieving the best result with more computing power and more input training material. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting epoch 1/100\n",
      "Epoch 1/100, Loss: 1.1488, Accuracy: 0.5349\n",
      "Evaluation Loss: 1.1366, Accuracy: 0.5373\n",
      "Starting epoch 2/100\n",
      "Epoch 2/100, Loss: 1.1288, Accuracy: 0.5391\n",
      "Evaluation Loss: 1.0786, Accuracy: 0.5695\n",
      "Starting epoch 3/100\n",
      "Epoch 3/100, Loss: 0.9407, Accuracy: 0.6161\n",
      "Evaluation Loss: 0.8669, Accuracy: 0.6403\n",
      "Starting epoch 4/100\n",
      "Epoch 4/100, Loss: 0.8454, Accuracy: 0.6483\n",
      "Evaluation Loss: 0.8212, Accuracy: 0.6595\n",
      "Starting epoch 5/100\n",
      "Epoch 5/100, Loss: 0.8142, Accuracy: 0.6617\n",
      "Evaluation Loss: 0.8018, Accuracy: 0.6709\n",
      "Starting epoch 6/100\n",
      "Epoch 6/100, Loss: 0.7923, Accuracy: 0.6720\n",
      "Evaluation Loss: 0.7822, Accuracy: 0.6775\n",
      "Starting epoch 7/100\n",
      "Epoch 7/100, Loss: 0.7752, Accuracy: 0.6798\n",
      "Evaluation Loss: 0.7730, Accuracy: 0.6814\n",
      "Starting epoch 8/100\n",
      "Epoch 8/100, Loss: 0.7608, Accuracy: 0.6862\n",
      "Evaluation Loss: 0.7733, Accuracy: 0.6832\n",
      "Starting epoch 9/100\n",
      "Epoch 9/100, Loss: 0.7478, Accuracy: 0.6923\n",
      "Evaluation Loss: 0.7650, Accuracy: 0.6889\n",
      "Starting epoch 10/100\n",
      "Epoch 10/100, Loss: 0.7352, Accuracy: 0.6984\n",
      "Evaluation Loss: 0.7557, Accuracy: 0.6903\n",
      "Starting epoch 11/100\n",
      "Epoch 11/100, Loss: 0.6938, Accuracy: 0.7147\n",
      "Evaluation Loss: 0.7432, Accuracy: 0.6986\n",
      "Starting epoch 12/100\n",
      "Epoch 12/100, Loss: 0.6839, Accuracy: 0.7192\n",
      "Evaluation Loss: 0.7444, Accuracy: 0.6980\n",
      "Starting epoch 13/100\n",
      "Epoch 13/100, Loss: 0.6774, Accuracy: 0.7220\n",
      "Evaluation Loss: 0.7476, Accuracy: 0.6978\n",
      "Starting epoch 14/100\n",
      "Epoch 14/100, Loss: 0.6721, Accuracy: 0.7240\n",
      "Evaluation Loss: 0.7462, Accuracy: 0.6974\n",
      "Starting epoch 15/100\n",
      "Epoch 15/100, Loss: 0.6672, Accuracy: 0.7263\n",
      "Evaluation Loss: 0.7477, Accuracy: 0.6973\n",
      "Starting epoch 16/100\n",
      "Epoch 16/100, Loss: 0.6618, Accuracy: 0.7291\n",
      "Evaluation Loss: 0.7525, Accuracy: 0.6972\n",
      "Starting epoch 17/100\n",
      "Epoch 17/100, Loss: 0.6562, Accuracy: 0.7316\n",
      "Evaluation Loss: 0.7563, Accuracy: 0.6954\n",
      "Starting epoch 18/100\n",
      "Epoch 18/100, Loss: 0.6509, Accuracy: 0.7338\n",
      "Evaluation Loss: 0.7576, Accuracy: 0.6948\n",
      "Starting epoch 19/100\n",
      "Epoch 19/100, Loss: 0.6451, Accuracy: 0.7359\n",
      "Evaluation Loss: 0.7651, Accuracy: 0.6935\n",
      "Starting epoch 20/100\n",
      "Epoch 20/100, Loss: 0.6384, Accuracy: 0.7389\n",
      "Evaluation Loss: 0.7610, Accuracy: 0.6953\n",
      "Starting epoch 21/100\n",
      "Epoch 21/100, Loss: 0.6262, Accuracy: 0.7450\n",
      "Evaluation Loss: 0.7698, Accuracy: 0.6939\n",
      "Early stopping at epoch 21\n",
      "Evaluation Loss: 0.7410, Accuracy: 0.7005\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.7410412680628196, 0.7004714631959893)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import torch.optim as optim\n",
    "import random\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, model_dim, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        pe = torch.zeros(max_len, model_dim)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, model_dim, 2).float() * (-np.log(10000.0) / model_dim)) \n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:, :x.size(1)]\n",
    "        return x\n",
    "\n",
    "class CustomTransformerEncoderLayer(nn.Module):\n",
    "    def __init__(self, model_dim, num_heads, ff_dim, dropout=0.1, activation='relu'):\n",
    "        super(CustomTransformerEncoderLayer, self).__init__()\n",
    "        self.self_attn = nn.MultiheadAttention(model_dim, num_heads, dropout=dropout, batch_first=True)\n",
    "        self.linear1 = nn.Linear(model_dim, ff_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.linear2 = nn.Linear(ff_dim, model_dim)\n",
    "        \n",
    "        # Choose activation function\n",
    "        if activation == 'relu':\n",
    "            self.activation = nn.ReLU()\n",
    "        elif activation == 'gelu':\n",
    "            self.activation = nn.GELU()\n",
    "        elif activation == 'softmax':\n",
    "            self.activation = nn.Softmax(dim=-1)\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported activation function: {activation}\")\n",
    "        \n",
    "        self.norm1 = nn.LayerNorm(model_dim)\n",
    "        self.norm2 = nn.LayerNorm(model_dim)\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, src):\n",
    "        src2 = self.self_attn(src, src, src)[0]\n",
    "        src = src + self.dropout1(src2)\n",
    "        src = self.norm1(src)\n",
    "        src2 = self.linear2(self.dropout(self.activation(self.linear1(src))))\n",
    "        src = src + self.dropout2(src2)\n",
    "        src = self.norm2(src)\n",
    "        return src\n",
    "\n",
    "class TransformerRNNHybridModel(nn.Module):\n",
    "    def __init__(self, input_dim, model_dim, num_heads, num_layers, ff_dim, rnn_hidden_dim, n_classes, dropout=0.1, activation='relu'):\n",
    "        super(TransformerRNNHybridModel, self).__init__()\n",
    "        self.model_dim = model_dim\n",
    "        self.input_projection = nn.Linear(input_dim, model_dim)  \n",
    "        self.pos_encoder = PositionalEncoding(model_dim)\n",
    "        \n",
    "        # Create custom transformer encoder layers\n",
    "        self.encoder_layers = nn.ModuleList([\n",
    "            CustomTransformerEncoderLayer(model_dim, num_heads, ff_dim, dropout, activation)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "        \n",
    "        self.rnn = nn.LSTM(model_dim, rnn_hidden_dim, num_layers=2, batch_first=True, dropout=dropout, bidirectional=True)  \n",
    "        self.attention = nn.MultiheadAttention(rnn_hidden_dim * 2, num_heads, dropout=dropout, batch_first=True)\n",
    "        \n",
    "        # Convolutional layers\n",
    "        self.conv1 = nn.Conv1d(in_channels=rnn_hidden_dim * 2, out_channels=64, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv1d(in_channels=64, out_channels=128, kernel_size=3, padding=1)\n",
    "        self.pool = nn.MaxPool1d(kernel_size=2)\n",
    "        \n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(128 * 25, 512),  \n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(512, n_classes)\n",
    "        )\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        initrange = 0.1\n",
    "        for layer in self.classifier:\n",
    "            if isinstance(layer, nn.Linear):\n",
    "                layer.bias.data.zero_()\n",
    "                layer.weight.data.uniform_(-initrange, initrange)\n",
    "\n",
    "    def forward(self, src):\n",
    "        src = self.input_projection(src) * np.sqrt(self.model_dim)  \n",
    "        src = self.pos_encoder(src)\n",
    "        for layer in self.encoder_layers:\n",
    "            src = layer(src)\n",
    "        rnn_output, _ = self.rnn(src)\n",
    "        attn_output, _ = self.attention(rnn_output, rnn_output, rnn_output)\n",
    "        \n",
    "        # Change the dimensions to fit the convolutional layers\n",
    "        conv_input = attn_output.permute(0, 2, 1)  \n",
    "        conv_output = self.pool(self.conv2(self.pool(self.conv1(conv_input))))\n",
    "        output = self.classifier(conv_output)\n",
    "        return output\n",
    "\n",
    "\n",
    "\n",
    "class PianoDataset(Dataset):\n",
    "    def __init__(self, csv_file, sequence_length=100, augment=False):\n",
    "        self.df = pd.read_csv(csv_file)\n",
    "        self.df = pd.concat([self.df.drop('event_type', axis=1), pd.get_dummies(self.df['event_type'], prefix='event_type')], axis=1)\n",
    "        self.df = self.df.astype(np.float32)  \n",
    "        self.sequence_length = sequence_length\n",
    "        self.augment = augment\n",
    "        self.data = self.generate_sequences()\n",
    "\n",
    "    def generate_sequences(self):\n",
    "        input_sequences = []\n",
    "        target_events = []\n",
    "        for i in range(len(self.df) - self.sequence_length):\n",
    "            full_sequence = self.df.iloc[i:i+self.sequence_length+1]\n",
    "            input_sequence = full_sequence.iloc[:-1]\n",
    "            target_event = full_sequence.iloc[-1]\n",
    "            input_sequences.append(input_sequence.values)\n",
    "            target_events.append(target_event.values.argmax())  \n",
    "        return list(zip(input_sequences, target_events))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sequence, target = self.data[idx]\n",
    "        if self.augment:\n",
    "            sequence = self.augment_sequence(sequence)\n",
    "        sequence_tensor = torch.tensor(sequence, dtype=torch.float32)\n",
    "        target_tensor = torch.tensor(target, dtype=torch.long)\n",
    "        return sequence_tensor, target_tensor\n",
    "\n",
    "    def augment_sequence(self, sequence):\n",
    "        # Transposition\n",
    "        if random.random() < 0.5:\n",
    "            transpose_amount = random.randint(-5, 5)\n",
    "            sequence[:, 0] += transpose_amount  \n",
    "        \n",
    "        # Time-stretching\n",
    "        if random.random() < 0.5:\n",
    "            stretch_factor = random.uniform(0.8, 1.2)\n",
    "            sequence[:, 1] *= stretch_factor  \n",
    "        \n",
    "        # Adding Noise\n",
    "        if random.random() < 0.5:\n",
    "            noise = np.random.normal(0, 0.01, sequence.shape)\n",
    "            sequence += noise\n",
    "        \n",
    "        return sequence\n",
    "\n",
    "def train_valid_test_split(dataset, train_ratio=0.7, valid_ratio=0.15, test_ratio=0.15):\n",
    "    assert train_ratio + valid_ratio + test_ratio == 1, \"Ratios must sum to 1\"\n",
    "    \n",
    "    train_size = int(train_ratio * len(dataset))\n",
    "    valid_size = int(valid_ratio * len(dataset))\n",
    "    test_size = len(dataset) - train_size - valid_size\n",
    "    \n",
    "    train_dataset, valid_dataset, test_dataset = random_split(dataset, [train_size, valid_size, test_size])\n",
    "    return train_dataset, valid_dataset, test_dataset\n",
    "\n",
    "# Define model parameters\n",
    "input_dim = 10  \n",
    "model_dim = 128  \n",
    "num_heads = 8  \n",
    "num_layers = 8  \n",
    "ff_dim = 512  \n",
    "rnn_hidden_dim = 256  \n",
    "n_classes = 8  \n",
    "sequence_length = 100\n",
    "\n",
    "# Set device and initialize model and optimizer\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "activation_function = 'relu'  \n",
    "model = TransformerRNNHybridModel(input_dim, model_dim, num_heads, num_layers, ff_dim, rnn_hidden_dim, n_classes, activation=activation_function).to(device)\n",
    "\n",
    "# Initialize weights\n",
    "def initialize_weights(m):\n",
    "    if isinstance(m, nn.Linear):\n",
    "        torch.nn.init.xavier_uniform_(m.weight)\n",
    "        m.bias.data.fill_(0.01)\n",
    "\n",
    "model.apply(initialize_weights)\n",
    "\n",
    "# Set optimizer with a lower learning rate and L2 regularization\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001, weight_decay=1e-5)\n",
    "\n",
    "# Define learning rate scheduler\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n",
    "\n",
    "# Define loss function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Create dataset and dataloaders\n",
    "output_csv = 'D:\\\\Projects\\\\Resources\\\\midis_v1.2\\\\Beethoven\\\\Beethoven_note_sequence.csv'\n",
    "full_dataset = PianoDataset(output_csv, sequence_length, augment=True)\n",
    "\n",
    "# Split the dataset\n",
    "train_dataset, valid_dataset, test_dataset = train_valid_test_split(full_dataset)\n",
    "\n",
    "# Create DataLoaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)  # Increased batch size\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=64, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# Training function with gradient clipping and early stopping\n",
    "def train_model(model, train_loader, valid_loader, optimizer, scheduler, num_epochs=50, patience=10):\n",
    "    model.train()\n",
    "    best_accuracy = 0.0\n",
    "    patience_counter = 0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss = 0.0\n",
    "        correct_predictions = 0\n",
    "        total_predictions = 0\n",
    "        print(f'Starting epoch {epoch+1}/{num_epochs}')\n",
    "        \n",
    "        for batch_idx, (inputs, targets) in enumerate(train_loader):\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            \n",
    "            # Ensure outputs and targets are correctly shaped\n",
    "            if outputs.shape[0] != targets.shape[0]:\n",
    "                print(f\"Shape mismatch: outputs shape {outputs.shape}, targets shape {targets.shape}\")\n",
    "                continue\n",
    "            \n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            # Apply gradient clipping\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item() * inputs.size(0)  \n",
    "            \n",
    "            # Calculate accuracy\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            correct_predictions += (predicted == targets).sum().item()\n",
    "            total_predictions += targets.size(0)\n",
    "\n",
    "        epoch_loss = running_loss / len(train_loader.dataset)\n",
    "        epoch_accuracy = correct_predictions / total_predictions\n",
    "        print(f'Epoch {epoch+1}/{num_epochs}, Loss: {epoch_loss:.4f}, Accuracy: {epoch_accuracy:.4f}')\n",
    "        \n",
    "        # Validate the model\n",
    "        val_loss, val_accuracy = evaluate_model(model, valid_loader)\n",
    "        \n",
    "        # Adjust learning rate\n",
    "        scheduler.step()\n",
    "        \n",
    "        # Early stopping\n",
    "        if val_accuracy > best_accuracy:\n",
    "            best_accuracy = val_accuracy\n",
    "            patience_counter = 0\n",
    "            torch.save(model.state_dict(), 'best_model_hybrid.pth')\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= patience:\n",
    "                print(f'Early stopping at epoch {epoch+1}')\n",
    "                break\n",
    "\n",
    "def evaluate_model(model, dataloader):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    running_loss = 0.0\n",
    "    correct_predictions = 0\n",
    "    total_predictions = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in dataloader:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            running_loss += loss.item() * inputs.size(0)  \n",
    "            \n",
    "            # Calculate accuracy\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            correct_predictions += (predicted == targets).sum().item()\n",
    "            total_predictions += targets.size(0)\n",
    "\n",
    "    total_loss = running_loss / len(dataloader.dataset)\n",
    "    accuracy = correct_predictions / total_predictions\n",
    "    print(f'Evaluation Loss: {total_loss:.4f}, Accuracy: {accuracy:.4f}')\n",
    "    model.train() \n",
    "    return total_loss, accuracy\n",
    "\n",
    "# Train the model with high patience early stopping and save the best model\n",
    "train_model(model, train_loader, valid_loader, optimizer, scheduler, num_epochs=100, patience=10)\n",
    "\n",
    "# Load the best model and evaluate on the test set\n",
    "model.load_state_dict(torch.load('best_model_hybrid.pth'))\n",
    "evaluate_model(model, test_loader)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
