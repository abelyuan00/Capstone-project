{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first part would be load the MIDI file and convert it to sequence that coule be directly used in the model. Here we only trained with only 12 piano notes (Beethoven's piano solo) for shorter training process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from mido import MidiFile\n",
    "import csv\n",
    "\n",
    "def midi_to_note_sequence_to_file(filepath, output_csv):\n",
    "    with open(output_csv, 'w', newline='') as csvfile:\n",
    "        fieldnames = ['piece_id','event_type', 'control', 'value', 'note', 'velocity', 'time_since_last_event', 'channel']\n",
    "        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "        \n",
    "        writer.writeheader()\n",
    "        piece_id = 0\n",
    "\n",
    "        for filename in os.listdir(filepath):\n",
    "            if filename.endswith('.mid') or filename.endswith('.midi'):\n",
    "                try:\n",
    "                    mid = MidiFile(os.path.join(filepath, filename))  \n",
    "                    for track in mid.tracks:\n",
    "                        time_since_last_event = 0\n",
    "                        for msg in track:\n",
    "                            time_since_last_event += msg.time\n",
    "                            if not msg.is_meta:\n",
    "                                if msg.type in ['note_on', 'note_off']:\n",
    "                                    event_type = 'note_start' if msg.type == 'note_on' and msg.velocity > 0 else 'note_stop'\n",
    "                                    writer.writerow({\n",
    "                                        'piece_id': piece_id,\n",
    "                                        'event_type': event_type,\n",
    "                                        'control': 0,\n",
    "                                        'value': 0,\n",
    "                                        'note': msg.note,\n",
    "                                        'velocity': msg.velocity,\n",
    "                                        'time_since_last_event': time_since_last_event,\n",
    "                                        'channel': msg.channel\n",
    "                                    })\n",
    "                                elif msg.type == 'control_change':\n",
    "                                    writer.writerow({\n",
    "                                        'piece_id': piece_id,\n",
    "                                        'event_type': 'control_change',\n",
    "                                        'control': msg.control,\n",
    "                                        'value': msg.value,\n",
    "                                        'note': 0,\n",
    "                                        'velocity': 0,\n",
    "                                        'time_since_last_event': time_since_last_event,\n",
    "                                        'channel': msg.channel\n",
    "                                    })\n",
    "                                time_since_last_event = 0\n",
    "                    piece_id += 1\n",
    "                except OSError as e:\n",
    "                    print(f\"Error processing {filename}: {e}\")\n",
    "\n",
    "path = 'D:\\\\Projects\\\\Resources\\\\midis_v1.2\\\\Beethoven'\n",
    "output_csv = 'D:\\\\Projects\\\\Resources\\\\midis_v1.2\\\\Beethoven\\\\Beethoven_note_sequence.csv'\n",
    "midi_to_note_sequence_to_file(path, output_csv)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the main training and evaluating below, as we can see from the result, the accuracy stuck at around 0.64"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Input Layer (Piano note sequence)\n",
    "     |\n",
    "     v\n",
    "Embedding Layer (Converts input tokens to dense vectors)\n",
    "     |\n",
    "     v\n",
    "Positional Encoding Layer (Adds positional information to embeddings)\n",
    "     |\n",
    "     v\n",
    "Transformer Encoder Layer 1 (Self-attention mechanism, layer normalization, residual connection, and feed-forward network)\n",
    "     |\n",
    "     v\n",
    "Transformer Encoder Layer 2 (Self-attention mechanism, layer normalization, residual connection, and feed-forward network)\n",
    "     |\n",
    "     v\n",
    "Transformer Encoder Layer N (Self-attention mechanism, layer normalization, residual connection, and feed-forward network)\n",
    "     |\n",
    "     v\n",
    "Flatten Layer (Converts sequence output to 1D)\n",
    "     |\n",
    "     v\n",
    "Fully Connected Layer 1 (Learns complex patterns)\n",
    "     |\n",
    "     v\n",
    "ReLU (Applies non-linearity)\n",
    "     |\n",
    "     v\n",
    "Dropout Layer (Prevents overfitting)\n",
    "     |\n",
    "     v\n",
    "Fully Connected Layer 2 (Learns more complex patterns)\n",
    "     |\n",
    "     v\n",
    "ReLU (Applies non-linearity)\n",
    "     |\n",
    "     v\n",
    "Output Layer (Softmax) (Produces probability distribution over classes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting epoch 1/100\n",
      "Epoch 1/100, Loss: 1.1528, Accuracy: 0.5343\n",
      "Evaluation Loss: 1.0568, Accuracy: 0.5845\n",
      "Starting epoch 2/100\n",
      "Epoch 2/100, Loss: 0.9910, Accuracy: 0.6004\n",
      "Evaluation Loss: 0.9564, Accuracy: 0.6062\n",
      "Starting epoch 3/100\n",
      "Epoch 3/100, Loss: 0.9346, Accuracy: 0.6166\n",
      "Evaluation Loss: 0.9250, Accuracy: 0.6175\n",
      "Starting epoch 4/100\n",
      "Epoch 4/100, Loss: 0.9124, Accuracy: 0.6240\n",
      "Evaluation Loss: 0.9245, Accuracy: 0.6198\n",
      "Starting epoch 5/100\n",
      "Epoch 5/100, Loss: 0.8970, Accuracy: 0.6296\n",
      "Evaluation Loss: 0.8977, Accuracy: 0.6306\n",
      "Starting epoch 6/100\n",
      "Epoch 6/100, Loss: 0.8863, Accuracy: 0.6337\n",
      "Evaluation Loss: 0.8944, Accuracy: 0.6297\n",
      "Starting epoch 7/100\n",
      "Epoch 7/100, Loss: 0.8763, Accuracy: 0.6382\n",
      "Evaluation Loss: 0.8845, Accuracy: 0.6371\n",
      "Starting epoch 8/100\n",
      "Epoch 8/100, Loss: 0.8669, Accuracy: 0.6423\n",
      "Evaluation Loss: 0.8804, Accuracy: 0.6355\n",
      "Starting epoch 9/100\n",
      "Epoch 9/100, Loss: 0.8588, Accuracy: 0.6457\n",
      "Evaluation Loss: 0.8798, Accuracy: 0.6355\n",
      "Starting epoch 10/100\n",
      "Epoch 10/100, Loss: 0.8505, Accuracy: 0.6494\n",
      "Evaluation Loss: 0.8704, Accuracy: 0.6410\n",
      "Starting epoch 11/100\n",
      "Epoch 11/100, Loss: 0.8441, Accuracy: 0.6520\n",
      "Evaluation Loss: 0.8723, Accuracy: 0.6453\n",
      "Starting epoch 12/100\n",
      "Epoch 12/100, Loss: 0.8355, Accuracy: 0.6558\n",
      "Evaluation Loss: 0.8833, Accuracy: 0.6367\n",
      "Starting epoch 13/100\n",
      "Epoch 13/100, Loss: 0.8273, Accuracy: 0.6596\n",
      "Evaluation Loss: 0.8669, Accuracy: 0.6433\n",
      "Starting epoch 14/100\n",
      "Epoch 14/100, Loss: 0.8193, Accuracy: 0.6634\n",
      "Evaluation Loss: 0.8783, Accuracy: 0.6391\n",
      "Starting epoch 15/100\n",
      "Epoch 15/100, Loss: 0.8104, Accuracy: 0.6668\n",
      "Evaluation Loss: 0.8697, Accuracy: 0.6424\n",
      "Starting epoch 16/100\n",
      "Epoch 16/100, Loss: 0.8016, Accuracy: 0.6707\n",
      "Evaluation Loss: 0.8676, Accuracy: 0.6439\n",
      "Starting epoch 17/100\n",
      "Epoch 17/100, Loss: 0.7929, Accuracy: 0.6747\n",
      "Evaluation Loss: 0.8745, Accuracy: 0.6436\n",
      "Starting epoch 18/100\n",
      "Epoch 18/100, Loss: 0.7844, Accuracy: 0.6784\n",
      "Evaluation Loss: 0.8743, Accuracy: 0.6433\n",
      "Starting epoch 19/100\n",
      "Epoch 19/100, Loss: 0.7746, Accuracy: 0.6830\n",
      "Evaluation Loss: 0.8799, Accuracy: 0.6452\n",
      "Starting epoch 20/100\n",
      "Epoch 20/100, Loss: 0.7647, Accuracy: 0.6866\n",
      "Evaluation Loss: 0.8836, Accuracy: 0.6425\n",
      "Starting epoch 21/100\n",
      "Epoch 21/100, Loss: 0.7555, Accuracy: 0.6909\n",
      "Evaluation Loss: 0.8917, Accuracy: 0.6360\n",
      "Starting epoch 22/100\n",
      "Epoch 22/100, Loss: 0.7444, Accuracy: 0.6956\n",
      "Evaluation Loss: 0.8960, Accuracy: 0.6402\n",
      "Starting epoch 23/100\n",
      "Epoch 23/100, Loss: 0.7332, Accuracy: 0.7000\n",
      "Evaluation Loss: 0.8966, Accuracy: 0.6371\n",
      "Starting epoch 24/100\n",
      "Epoch 24/100, Loss: 0.7216, Accuracy: 0.7061\n",
      "Evaluation Loss: 0.9035, Accuracy: 0.6324\n",
      "Starting epoch 25/100\n",
      "Epoch 25/100, Loss: 0.7099, Accuracy: 0.7105\n",
      "Evaluation Loss: 0.9108, Accuracy: 0.6347\n",
      "Starting epoch 26/100\n",
      "Epoch 26/100, Loss: 0.6976, Accuracy: 0.7162\n",
      "Evaluation Loss: 0.9288, Accuracy: 0.6341\n",
      "Early stopping at epoch 26\n",
      "Evaluation Loss: 0.8742, Accuracy: 0.6438\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.8742001934206507, 0.6438128755934792)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import torch.optim as optim\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, model_dim, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        pe = torch.zeros(max_len, model_dim)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, model_dim, 2).float() * (-np.log(10000.0) / model_dim))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:, :x.size(1)]\n",
    "        return x\n",
    "\n",
    "class CustomTransformerEncoderLayer(nn.Module):\n",
    "    def __init__(self, model_dim, num_heads, ff_dim, dropout=0.1, activation='relu'):\n",
    "        super(CustomTransformerEncoderLayer, self).__init__()\n",
    "        self.self_attn = nn.MultiheadAttention(model_dim, num_heads, dropout=dropout, batch_first=True)\n",
    "        self.linear1 = nn.Linear(model_dim, ff_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.linear2 = nn.Linear(ff_dim, model_dim)\n",
    "        \n",
    "        # Choose activation function\n",
    "        if activation == 'relu':\n",
    "            self.activation = nn.ReLU()\n",
    "        elif activation == 'gelu':\n",
    "            self.activation = nn.GELU()\n",
    "        elif activation == 'softmax':\n",
    "            self.activation = nn.Softmax(dim=-1)\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported activation function: {activation}\")\n",
    "        \n",
    "        self.norm1 = nn.LayerNorm(model_dim)\n",
    "        self.norm2 = nn.LayerNorm(model_dim)\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, src):\n",
    "        src2 = self.self_attn(src, src, src)[0]\n",
    "        src = src + self.dropout1(src2)\n",
    "        src = self.norm1(src)\n",
    "        src2 = self.linear2(self.dropout(self.activation(self.linear1(src))))\n",
    "        src = src + self.dropout2(src2)\n",
    "        src = self.norm2(src)\n",
    "        return src\n",
    "\n",
    "class CustomTransformerModel(nn.Module):\n",
    "    def __init__(self, input_dim, model_dim, num_heads, num_layers, ff_dim, n_classes, dropout=0.1, activation='relu'):\n",
    "        super(CustomTransformerModel, self).__init__()\n",
    "        self.model_dim = model_dim\n",
    "        self.input_projection = nn.Linear(input_dim, model_dim)  \n",
    "        self.pos_encoder = PositionalEncoding(model_dim)\n",
    "        \n",
    "        # Create custom transformer encoder layers\n",
    "        self.encoder_layers = nn.ModuleList([\n",
    "            CustomTransformerEncoderLayer(model_dim, num_heads, ff_dim, dropout, activation)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "        \n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(in_features=model_dim * sequence_length, out_features=n_classes)\n",
    "        )\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        initrange = 0.1\n",
    "        for layer in self.classifier:\n",
    "            if isinstance(layer, nn.Linear):\n",
    "                layer.bias.data.zero_()\n",
    "                layer.weight.data.uniform_(-initrange, initrange)\n",
    "\n",
    "    def forward(self, src):\n",
    "        src = self.input_projection(src) * np.sqrt(self.model_dim)  # Project input and scale\n",
    "        src = self.pos_encoder(src)\n",
    "        for layer in self.encoder_layers:\n",
    "            src = layer(src)\n",
    "        output = self.classifier(src)\n",
    "        return output\n",
    "\n",
    "class PianoDataset(Dataset):\n",
    "    def __init__(self, csv_file, sequence_length=100):\n",
    "        self.df = pd.read_csv(csv_file)\n",
    "        self.df = pd.concat([self.df.drop('event_type', axis=1), pd.get_dummies(self.df['event_type'], prefix='event_type')], axis=1)\n",
    "        self.df = self.df.astype(np.float32)  \n",
    "        self.sequence_length = sequence_length\n",
    "        self.data = self.generate_sequences()\n",
    "\n",
    "    def generate_sequences(self):\n",
    "        input_sequences = []\n",
    "        target_events = []\n",
    "        for i in range(len(self.df) - self.sequence_length):\n",
    "            full_sequence = self.df.iloc[i:i+self.sequence_length+1]\n",
    "            input_sequence = full_sequence.iloc[:-1]\n",
    "            target_event = full_sequence.iloc[-1]\n",
    "            input_sequences.append(input_sequence.values)\n",
    "            target_events.append(target_event.values.argmax())  # Use class index as target\n",
    "        return list(zip(input_sequences, target_events))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sequence, target = self.data[idx]\n",
    "        sequence_tensor = torch.tensor(sequence, dtype=torch.float32)\n",
    "        target_tensor = torch.tensor(target, dtype=torch.long)\n",
    "        return sequence_tensor, target_tensor\n",
    "\n",
    "def train_valid_test_split(dataset, train_ratio=0.7, valid_ratio=0.15, test_ratio=0.15):\n",
    "    assert train_ratio + valid_ratio + test_ratio == 1, \"Ratios must sum to 1\"\n",
    "    \n",
    "    train_size = int(train_ratio * len(dataset))\n",
    "    valid_size = int(valid_ratio * len(dataset))\n",
    "    test_size = len(dataset) - train_size - valid_size\n",
    "    \n",
    "    train_dataset, valid_dataset, test_dataset = random_split(dataset, [train_size, valid_size, test_size])\n",
    "    return train_dataset, valid_dataset, test_dataset\n",
    "\n",
    "# Define model parameters\n",
    "input_dim = 10  \n",
    "model_dim = 128  \n",
    "num_heads = 4  \n",
    "num_layers = 8  \n",
    "ff_dim = 512  \n",
    "n_classes = 8  \n",
    "sequence_length = 100\n",
    "\n",
    "# Set device and initialize model and optimizer\n",
    "device = torch.device(\"cuda\")\n",
    "activation_function = 'gelu'\n",
    "model = CustomTransformerModel(input_dim, model_dim, num_heads, num_layers, ff_dim, n_classes, activation=activation_function).to(device)\n",
    "\n",
    "# Initialize weights\n",
    "def initialize_weights(m):\n",
    "    if isinstance(m, nn.Linear):\n",
    "        torch.nn.init.xavier_uniform_(m.weight)\n",
    "        m.bias.data.fill_(0.01)\n",
    "\n",
    "model.apply(initialize_weights)\n",
    "\n",
    "# Set optimizer with a lower learning rate\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
    "\n",
    "# Define loss function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Create dataset and dataloaders\n",
    "output_csv = 'D:\\\\Projects\\\\Resources\\\\midis_v1.2\\\\Beethoven\\\\Beethoven_note_sequence.csv'\n",
    "full_dataset = PianoDataset(output_csv, sequence_length)\n",
    "\n",
    "# Split the dataset\n",
    "train_dataset, valid_dataset, test_dataset = train_valid_test_split(full_dataset)\n",
    "\n",
    "# Create DataLoaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=512, shuffle=True)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=512, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# Training function with gradient clipping and early stopping\n",
    "def train_model(model, train_loader, valid_loader, optimizer, num_epochs=100, patience=5):\n",
    "    model.train()\n",
    "    best_accuracy = 0.0\n",
    "    patience_counter = 0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss = 0.0\n",
    "        correct_predictions = 0\n",
    "        total_predictions = 0\n",
    "        print(f'Starting epoch {epoch+1}/{num_epochs}')\n",
    "        \n",
    "        for batch_idx, (inputs, targets) in enumerate(train_loader):\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            \n",
    "            # Ensure outputs and targets are correctly shaped\n",
    "            if outputs.shape[0] != targets.shape[0]:\n",
    "                print(f\"Shape mismatch: outputs shape {outputs.shape}, targets shape {targets.shape}\")\n",
    "                continue\n",
    "            \n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item() * inputs.size(0)  \n",
    "            \n",
    "            # Calculate accuracy\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            correct_predictions += (predicted == targets).sum().item()\n",
    "            total_predictions += targets.size(0)\n",
    "\n",
    "        epoch_loss = running_loss / len(train_loader.dataset)\n",
    "        epoch_accuracy = correct_predictions / total_predictions\n",
    "        print(f'Epoch {epoch+1}/{num_epochs}, Loss: {epoch_loss:.4f}, Accuracy: {epoch_accuracy:.4f}')\n",
    "        \n",
    "        # Validate the model\n",
    "        val_loss, val_accuracy = evaluate_model(model, valid_loader)\n",
    "        \n",
    "        # Early stopping\n",
    "        if val_accuracy > best_accuracy:\n",
    "            best_accuracy = val_accuracy\n",
    "            patience_counter = 0\n",
    "            torch.save(model.state_dict(), 'best_model.pth')\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= patience:\n",
    "                print(f'Early stopping at epoch {epoch+1}')\n",
    "                break\n",
    "\n",
    "def evaluate_model(model, dataloader):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    correct_predictions = 0\n",
    "    total_predictions = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in dataloader:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            running_loss += loss.item() * inputs.size(0)  \n",
    "            \n",
    "            # Calculate accuracy\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            correct_predictions += (predicted == targets).sum().item()\n",
    "            total_predictions += targets.size(0)\n",
    "\n",
    "    total_loss = running_loss / len(dataloader.dataset)\n",
    "    accuracy = correct_predictions / total_predictions\n",
    "    print(f'Evaluation Loss: {total_loss:.4f}, Accuracy: {accuracy:.4f}')\n",
    "    return total_loss, accuracy\n",
    "\n",
    "# Train the model with high patience early stopping and save the best model\n",
    "train_model(model, train_loader, valid_loader, optimizer, num_epochs=100, patience=15)\n",
    "\n",
    "# Load the best model and evaluate on the test set\n",
    "model.load_state_dict(torch.load('best_model.pth'))\n",
    "evaluate_model(model, test_loader)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can try to modify the model with some approaches in cluding data augmenting and normalization and pretraining. But even with tuning, the accuracy are stuck. I would presume that large sample and longer-training with more complex structure transformer can achieve higher accuracy, but for the limited computational power, we might need to integrate some other models.p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Difference: \n",
    "Layer normalization is applied before the residual connection.\n",
    "Dropout rate is set to 0.2.\n",
    "Activation function is gelu.\n",
    "\n",
    "Input Layer (Piano note sequence)\n",
    "     |\n",
    "     v\n",
    "Embedding Layer (Converts input tokens to dense vectors)\n",
    "     |\n",
    "     v\n",
    "Positional Encoding Layer (Adds positional information to embeddings)\n",
    "     |\n",
    "     v\n",
    "Transformer Encoder Layer 1 (Self-attention mechanism, layer normalization, residual connection, and feed-forward network)\n",
    "     |\n",
    "     v\n",
    "Transformer Encoder Layer 2 (Self-attention mechanism, layer normalization, residual connection, and feed-forward network)\n",
    "     |\n",
    "     v\n",
    "Transformer Encoder Layer N (Self-attention mechanism, layer normalization, residual connection, and feed-forward network)\n",
    "     |\n",
    "     v\n",
    "Flatten Layer (Converts sequence output to 1D)\n",
    "     |\n",
    "     v\n",
    "Fully Connected Layer 1 (Learns complex patterns)\n",
    "     |\n",
    "     v\n",
    "ReLU (Applies non-linearity)\n",
    "     |\n",
    "     v\n",
    "Dropout Layer (Prevents overfitting)\n",
    "     |\n",
    "     v\n",
    "Fully Connected Layer 2 (Learns more complex patterns)\n",
    "     |\n",
    "     v\n",
    "ReLU (Applies non-linearity)\n",
    "     |\n",
    "     v\n",
    "Output Layer (Softmax) (Produces probability distribution over classes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting epoch 1/100\n",
      "Epoch 1/100, Loss: 19.5316, Accuracy: 0.4236\n",
      "Evaluation Loss: 13.8370, Accuracy: 0.5156\n",
      "Starting epoch 2/100\n",
      "Epoch 2/100, Loss: 10.7625, Accuracy: 0.4356\n",
      "Evaluation Loss: 18.0568, Accuracy: 0.2750\n",
      "Starting epoch 3/100\n",
      "Epoch 3/100, Loss: 10.0768, Accuracy: 0.4419\n",
      "Evaluation Loss: 10.0024, Accuracy: 0.3313\n",
      "Starting epoch 4/100\n",
      "Epoch 4/100, Loss: 8.8451, Accuracy: 0.4564\n",
      "Evaluation Loss: 8.7254, Accuracy: 0.5268\n",
      "Starting epoch 5/100\n",
      "Epoch 5/100, Loss: 7.7756, Accuracy: 0.4656\n",
      "Evaluation Loss: 7.4704, Accuracy: 0.4753\n",
      "Starting epoch 6/100\n",
      "Epoch 6/100, Loss: 6.8677, Accuracy: 0.4781\n",
      "Evaluation Loss: 6.0451, Accuracy: 0.4756\n",
      "Starting epoch 7/100\n",
      "Epoch 7/100, Loss: 5.8033, Accuracy: 0.4880\n",
      "Evaluation Loss: 5.5495, Accuracy: 0.4361\n",
      "Starting epoch 8/100\n",
      "Epoch 8/100, Loss: 4.6672, Accuracy: 0.4979\n",
      "Evaluation Loss: 4.6682, Accuracy: 0.4864\n",
      "Starting epoch 9/100\n",
      "Epoch 9/100, Loss: 3.9401, Accuracy: 0.5066\n",
      "Evaluation Loss: 3.8136, Accuracy: 0.5049\n",
      "Starting epoch 10/100\n",
      "Epoch 10/100, Loss: 3.3335, Accuracy: 0.5174\n",
      "Evaluation Loss: 3.3095, Accuracy: 0.5157\n",
      "Starting epoch 11/100\n",
      "Epoch 11/100, Loss: 2.8958, Accuracy: 0.5251\n",
      "Evaluation Loss: 2.8848, Accuracy: 0.5356\n",
      "Starting epoch 12/100\n",
      "Epoch 12/100, Loss: 2.4997, Accuracy: 0.5351\n",
      "Evaluation Loss: 2.5920, Accuracy: 0.5584\n",
      "Starting epoch 13/100\n",
      "Epoch 13/100, Loss: 2.1701, Accuracy: 0.5456\n",
      "Evaluation Loss: 2.4153, Accuracy: 0.5409\n",
      "Starting epoch 14/100\n",
      "Epoch 14/100, Loss: 2.0339, Accuracy: 0.5547\n",
      "Evaluation Loss: 2.2925, Accuracy: 0.5483\n",
      "Starting epoch 15/100\n",
      "Epoch 15/100, Loss: 1.9401, Accuracy: 0.5641\n",
      "Evaluation Loss: 2.1368, Accuracy: 0.5755\n",
      "Starting epoch 16/100\n",
      "Epoch 16/100, Loss: 1.7954, Accuracy: 0.5712\n",
      "Evaluation Loss: 2.0664, Accuracy: 0.5562\n",
      "Starting epoch 17/100\n",
      "Epoch 17/100, Loss: 1.7189, Accuracy: 0.5770\n",
      "Evaluation Loss: 1.7970, Accuracy: 0.5724\n",
      "Starting epoch 18/100\n",
      "Epoch 18/100, Loss: 1.6195, Accuracy: 0.5829\n",
      "Evaluation Loss: 1.5812, Accuracy: 0.5834\n",
      "Starting epoch 19/100\n",
      "Epoch 19/100, Loss: 1.5078, Accuracy: 0.5874\n",
      "Evaluation Loss: 1.5874, Accuracy: 0.5864\n",
      "Starting epoch 20/100\n",
      "Epoch 20/100, Loss: 1.3994, Accuracy: 0.5914\n",
      "Evaluation Loss: 1.4598, Accuracy: 0.5865\n",
      "Starting epoch 21/100\n",
      "Epoch 21/100, Loss: 1.3425, Accuracy: 0.5950\n",
      "Evaluation Loss: 1.3718, Accuracy: 0.5943\n",
      "Starting epoch 22/100\n",
      "Epoch 22/100, Loss: 1.2498, Accuracy: 0.6000\n",
      "Evaluation Loss: 1.3579, Accuracy: 0.5886\n",
      "Starting epoch 23/100\n",
      "Epoch 23/100, Loss: 1.1997, Accuracy: 0.6038\n",
      "Evaluation Loss: 1.1738, Accuracy: 0.6031\n",
      "Starting epoch 24/100\n",
      "Epoch 24/100, Loss: 1.1110, Accuracy: 0.6066\n",
      "Evaluation Loss: 1.2236, Accuracy: 0.5954\n",
      "Starting epoch 25/100\n",
      "Epoch 25/100, Loss: 1.0767, Accuracy: 0.6084\n",
      "Evaluation Loss: 1.0924, Accuracy: 0.5954\n",
      "Starting epoch 26/100\n",
      "Epoch 26/100, Loss: 1.0230, Accuracy: 0.6099\n",
      "Evaluation Loss: 1.0716, Accuracy: 0.6070\n",
      "Starting epoch 27/100\n",
      "Epoch 27/100, Loss: 0.9773, Accuracy: 0.6142\n",
      "Evaluation Loss: 1.0279, Accuracy: 0.6071\n",
      "Starting epoch 28/100\n",
      "Epoch 28/100, Loss: 0.9639, Accuracy: 0.6169\n",
      "Evaluation Loss: 1.0175, Accuracy: 0.6078\n",
      "Starting epoch 29/100\n",
      "Epoch 29/100, Loss: 0.9465, Accuracy: 0.6198\n",
      "Evaluation Loss: 1.0232, Accuracy: 0.6018\n",
      "Starting epoch 30/100\n",
      "Epoch 30/100, Loss: 0.9426, Accuracy: 0.6226\n",
      "Evaluation Loss: 1.0256, Accuracy: 0.6133\n",
      "Starting epoch 31/100\n",
      "Epoch 31/100, Loss: 0.9303, Accuracy: 0.6252\n",
      "Evaluation Loss: 0.9962, Accuracy: 0.6192\n",
      "Starting epoch 32/100\n",
      "Epoch 32/100, Loss: 0.9090, Accuracy: 0.6288\n",
      "Evaluation Loss: 0.9979, Accuracy: 0.6188\n",
      "Starting epoch 33/100\n",
      "Epoch 33/100, Loss: 0.9019, Accuracy: 0.6310\n",
      "Evaluation Loss: 1.0105, Accuracy: 0.6193\n",
      "Starting epoch 34/100\n",
      "Epoch 34/100, Loss: 0.8957, Accuracy: 0.6337\n",
      "Evaluation Loss: 1.0087, Accuracy: 0.6136\n",
      "Starting epoch 35/100\n",
      "Epoch 35/100, Loss: 0.8875, Accuracy: 0.6361\n",
      "Evaluation Loss: 1.0063, Accuracy: 0.6248\n",
      "Starting epoch 36/100\n",
      "Epoch 36/100, Loss: 0.8819, Accuracy: 0.6375\n",
      "Evaluation Loss: 1.0016, Accuracy: 0.6287\n",
      "Starting epoch 37/100\n",
      "Epoch 37/100, Loss: 0.8782, Accuracy: 0.6403\n",
      "Evaluation Loss: 1.0057, Accuracy: 0.6208\n",
      "Starting epoch 38/100\n",
      "Epoch 38/100, Loss: 0.8742, Accuracy: 0.6425\n",
      "Evaluation Loss: 1.0155, Accuracy: 0.6258\n",
      "Starting epoch 39/100\n",
      "Epoch 39/100, Loss: 0.8673, Accuracy: 0.6442\n",
      "Evaluation Loss: 1.0186, Accuracy: 0.6211\n",
      "Starting epoch 40/100\n",
      "Epoch 40/100, Loss: 0.8635, Accuracy: 0.6467\n",
      "Evaluation Loss: 1.0212, Accuracy: 0.6250\n",
      "Starting epoch 41/100\n",
      "Epoch 41/100, Loss: 0.8586, Accuracy: 0.6495\n",
      "Evaluation Loss: 1.0383, Accuracy: 0.6196\n",
      "Starting epoch 42/100\n",
      "Epoch 42/100, Loss: 0.8526, Accuracy: 0.6494\n",
      "Evaluation Loss: 1.0233, Accuracy: 0.6331\n",
      "Starting epoch 43/100\n",
      "Epoch 43/100, Loss: 0.8476, Accuracy: 0.6519\n",
      "Evaluation Loss: 1.0300, Accuracy: 0.6323\n",
      "Starting epoch 44/100\n",
      "Epoch 44/100, Loss: 0.8434, Accuracy: 0.6551\n",
      "Evaluation Loss: 1.0343, Accuracy: 0.6355\n",
      "Starting epoch 45/100\n",
      "Epoch 45/100, Loss: 0.8410, Accuracy: 0.6559\n",
      "Evaluation Loss: 1.0287, Accuracy: 0.6350\n",
      "Starting epoch 46/100\n",
      "Epoch 46/100, Loss: 0.8365, Accuracy: 0.6578\n",
      "Evaluation Loss: 1.0484, Accuracy: 0.6204\n",
      "Starting epoch 47/100\n",
      "Epoch 47/100, Loss: 0.8298, Accuracy: 0.6596\n",
      "Evaluation Loss: 1.0462, Accuracy: 0.6246\n",
      "Starting epoch 48/100\n",
      "Epoch 48/100, Loss: 0.8282, Accuracy: 0.6617\n",
      "Evaluation Loss: 1.0612, Accuracy: 0.6340\n",
      "Starting epoch 49/100\n",
      "Epoch 49/100, Loss: 0.8204, Accuracy: 0.6635\n",
      "Evaluation Loss: 1.0639, Accuracy: 0.6374\n",
      "Starting epoch 50/100\n",
      "Epoch 50/100, Loss: 0.8182, Accuracy: 0.6647\n",
      "Evaluation Loss: 1.0671, Accuracy: 0.6332\n",
      "Starting epoch 51/100\n",
      "Epoch 51/100, Loss: 0.8133, Accuracy: 0.6673\n",
      "Evaluation Loss: 1.0838, Accuracy: 0.6250\n",
      "Starting epoch 52/100\n",
      "Epoch 52/100, Loss: 0.8079, Accuracy: 0.6685\n",
      "Evaluation Loss: 1.0804, Accuracy: 0.6373\n",
      "Starting epoch 53/100\n",
      "Epoch 53/100, Loss: 0.8042, Accuracy: 0.6708\n",
      "Evaluation Loss: 1.0662, Accuracy: 0.6360\n",
      "Starting epoch 54/100\n",
      "Epoch 54/100, Loss: 0.8012, Accuracy: 0.6722\n",
      "Evaluation Loss: 1.1037, Accuracy: 0.6179\n",
      "Starting epoch 55/100\n",
      "Epoch 55/100, Loss: 0.7955, Accuracy: 0.6743\n",
      "Evaluation Loss: 1.0734, Accuracy: 0.6271\n",
      "Starting epoch 56/100\n",
      "Epoch 56/100, Loss: 0.7930, Accuracy: 0.6750\n",
      "Evaluation Loss: 1.0725, Accuracy: 0.6379\n",
      "Starting epoch 57/100\n",
      "Epoch 57/100, Loss: 0.7867, Accuracy: 0.6784\n",
      "Evaluation Loss: 1.0708, Accuracy: 0.6318\n",
      "Starting epoch 58/100\n",
      "Epoch 58/100, Loss: 0.7828, Accuracy: 0.6796\n",
      "Evaluation Loss: 1.0682, Accuracy: 0.6341\n",
      "Starting epoch 59/100\n",
      "Epoch 59/100, Loss: 0.7773, Accuracy: 0.6815\n",
      "Evaluation Loss: 1.0714, Accuracy: 0.6383\n",
      "Starting epoch 60/100\n",
      "Epoch 60/100, Loss: 0.7726, Accuracy: 0.6833\n",
      "Evaluation Loss: 1.0832, Accuracy: 0.6373\n",
      "Starting epoch 61/100\n",
      "Epoch 61/100, Loss: 0.7691, Accuracy: 0.6850\n",
      "Evaluation Loss: 1.0974, Accuracy: 0.6299\n",
      "Starting epoch 62/100\n",
      "Epoch 62/100, Loss: 0.7657, Accuracy: 0.6866\n",
      "Evaluation Loss: 1.1107, Accuracy: 0.6301\n",
      "Starting epoch 63/100\n",
      "Epoch 63/100, Loss: 0.7600, Accuracy: 0.6894\n",
      "Evaluation Loss: 1.0972, Accuracy: 0.6277\n",
      "Starting epoch 64/100\n",
      "Epoch 64/100, Loss: 0.7567, Accuracy: 0.6908\n",
      "Evaluation Loss: 1.0973, Accuracy: 0.6378\n",
      "Starting epoch 65/100\n",
      "Epoch 65/100, Loss: 0.7510, Accuracy: 0.6929\n",
      "Evaluation Loss: 1.0981, Accuracy: 0.6364\n",
      "Starting epoch 66/100\n",
      "Epoch 66/100, Loss: 0.7471, Accuracy: 0.6943\n",
      "Evaluation Loss: 1.1056, Accuracy: 0.6347\n",
      "Starting epoch 67/100\n",
      "Epoch 67/100, Loss: 0.7407, Accuracy: 0.6969\n",
      "Evaluation Loss: 1.1081, Accuracy: 0.6348\n",
      "Starting epoch 68/100\n",
      "Epoch 68/100, Loss: 0.7389, Accuracy: 0.6983\n",
      "Evaluation Loss: 1.1021, Accuracy: 0.6276\n",
      "Starting epoch 69/100\n",
      "Epoch 69/100, Loss: 0.7333, Accuracy: 0.7005\n",
      "Evaluation Loss: 1.1109, Accuracy: 0.6354\n",
      "Starting epoch 70/100\n",
      "Epoch 70/100, Loss: 0.7278, Accuracy: 0.7021\n",
      "Evaluation Loss: 1.1249, Accuracy: 0.6297\n",
      "Starting epoch 71/100\n",
      "Epoch 71/100, Loss: 0.7232, Accuracy: 0.7044\n",
      "Evaluation Loss: 1.1312, Accuracy: 0.6336\n",
      "Starting epoch 72/100\n",
      "Epoch 72/100, Loss: 0.7184, Accuracy: 0.7064\n",
      "Evaluation Loss: 1.1255, Accuracy: 0.6369\n",
      "Starting epoch 73/100\n",
      "Epoch 73/100, Loss: 0.7145, Accuracy: 0.7087\n",
      "Evaluation Loss: 1.1397, Accuracy: 0.6234\n",
      "Starting epoch 74/100\n",
      "Epoch 74/100, Loss: 0.7070, Accuracy: 0.7108\n",
      "Evaluation Loss: 1.1412, Accuracy: 0.6283\n",
      "Early stopping at epoch 74\n",
      "Evaluation Loss: 1.0177, Accuracy: 0.6411\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1.0177395746501232, 0.6411484445034695)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import torch.optim as optim\n",
    "import random\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, model_dim, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        pe = torch.zeros(max_len, model_dim)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, model_dim, 2).float() * (-np.log(10000.0) / model_dim))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:, :x.size(1)]\n",
    "        return x\n",
    "\n",
    "class CustomTransformerEncoderLayer(nn.Module):\n",
    "    def __init__(self, model_dim, num_heads, ff_dim, dropout=0.1, activation='gelu'):\n",
    "        super(CustomTransformerEncoderLayer, self).__init__()\n",
    "        self.self_attn = nn.MultiheadAttention(model_dim, num_heads, dropout=dropout, batch_first=True)\n",
    "        self.linear1 = nn.Linear(model_dim, ff_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.linear2 = nn.Linear(ff_dim, model_dim)\n",
    "        \n",
    "        # Choose activation function\n",
    "        if activation == 'relu':\n",
    "            self.activation = nn.ReLU()\n",
    "        elif activation == 'gelu':\n",
    "            self.activation = nn.GELU()\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported activation function: {activation}\")\n",
    "        \n",
    "        self.norm1 = nn.LayerNorm(model_dim)\n",
    "        self.norm2 = nn.LayerNorm(model_dim)\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, src):\n",
    "        # Pre-layer norm\n",
    "        src2 = self.norm1(src)\n",
    "        src2 = self.self_attn(src2, src2, src2)[0]\n",
    "        src = src + self.dropout1(src2)\n",
    "        \n",
    "        # Pre-layer norm\n",
    "        src2 = self.norm2(src)\n",
    "        src2 = self.linear2(self.dropout(self.activation(self.linear1(src2))))\n",
    "        src = src + self.dropout2(src2)\n",
    "        return src\n",
    "\n",
    "class CustomTransformerModel(nn.Module):\n",
    "    def __init__(self, input_dim, model_dim, num_heads, num_layers, ff_dim, n_classes, sequence_length, dropout=0.1, activation='gelu'):\n",
    "        super(CustomTransformerModel, self).__init__()\n",
    "        self.model_dim = model_dim\n",
    "        self.input_projection = nn.Linear(input_dim, model_dim)  # Project input to model dimension\n",
    "        self.pos_encoder = PositionalEncoding(model_dim)\n",
    "        \n",
    "        # Create custom transformer encoder layers\n",
    "        self.encoder_layers = nn.ModuleList([\n",
    "            CustomTransformerEncoderLayer(model_dim, num_heads, ff_dim, dropout, activation)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "        \n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(in_features=model_dim * sequence_length, out_features=n_classes)\n",
    "        )\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        initrange = 0.1\n",
    "        for layer in self.classifier:\n",
    "            if isinstance(layer, nn.Linear):\n",
    "                layer.bias.data.zero_()\n",
    "                layer.weight.data.uniform_(-initrange, initrange)\n",
    "\n",
    "    def forward(self, src):\n",
    "        src = self.input_projection(src) * np.sqrt(self.model_dim)  # Project input and scale\n",
    "        src = self.pos_encoder(src)\n",
    "        for layer in self.encoder_layers:\n",
    "            src = layer(src)\n",
    "        output = self.classifier(src)\n",
    "        return output\n",
    "\n",
    "class PianoDataset(Dataset):\n",
    "    def __init__(self, csv_file, sequence_length=100, augment=False):\n",
    "        self.df = pd.read_csv(csv_file)\n",
    "        self.df = pd.concat([self.df.drop('event_type', axis=1), pd.get_dummies(self.df['event_type'], prefix='event_type')], axis=1)\n",
    "        self.df = self.df.astype(np.float32)  # Ensure all data is of type float32\n",
    "        self.sequence_length = sequence_length\n",
    "        self.augment = augment\n",
    "        self.data = self.generate_sequences()\n",
    "\n",
    "    def generate_sequences(self):\n",
    "        input_sequences = []\n",
    "        target_events = []\n",
    "        for i in range(len(self.df) - self.sequence_length):\n",
    "            full_sequence = self.df.iloc[i:i+self.sequence_length+1]\n",
    "            input_sequence = full_sequence.iloc[:-1]\n",
    "            target_event = full_sequence.iloc[-1]\n",
    "            input_sequences.append(input_sequence.values)\n",
    "            target_events.append(target_event.values.argmax())  # Use class index as target\n",
    "        return list(zip(input_sequences, target_events))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sequence, target = self.data[idx]\n",
    "        if self.augment:\n",
    "            sequence = self.augment_sequence(sequence)\n",
    "        sequence_tensor = torch.tensor(sequence, dtype=torch.float32)\n",
    "        target_tensor = torch.tensor(target, dtype=torch.long)\n",
    "        return sequence_tensor, target_tensor\n",
    "\n",
    "    def augment_sequence(self, sequence):\n",
    "        # Transposition\n",
    "        if random.random() < 0.5:\n",
    "            transpose_amount = random.randint(-5, 5)\n",
    "            sequence[:, 0] += transpose_amount  \n",
    "        \n",
    "        # Time-stretching\n",
    "        if random.random() < 0.5:\n",
    "            stretch_factor = random.uniform(0.8, 1.2)\n",
    "            sequence[:, 1] *= stretch_factor  \n",
    "        \n",
    "        # Adding Noise\n",
    "        if random.random() < 0.5:\n",
    "            noise = np.random.normal(0, 0.01, sequence.shape)\n",
    "            sequence += noise\n",
    "        \n",
    "        return sequence\n",
    "\n",
    "def train_valid_test_split(dataset, train_ratio=0.7, valid_ratio=0.15, test_ratio=0.15):\n",
    "    assert train_ratio + valid_ratio + test_ratio == 1, \"Ratios must sum to 1\"\n",
    "    \n",
    "    train_size = int(train_ratio * len(dataset))\n",
    "    valid_size = int(valid_ratio * len(dataset))\n",
    "    test_size = len(dataset) - train_size - valid_size\n",
    "    \n",
    "    train_dataset, valid_dataset, test_dataset = random_split(dataset, [train_size, valid_size, test_size])\n",
    "    return train_dataset, valid_dataset, test_dataset\n",
    "\n",
    "# Define model parameters\n",
    "input_dim = 10  \n",
    "model_dim = 128  \n",
    "num_heads = 8  \n",
    "num_layers = 12  \n",
    "ff_dim = 512  \n",
    "n_classes = 8  \n",
    "sequence_length = 100\n",
    "\n",
    "# Set device and initialize model and optimizer\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "activation_function = 'gelu'  # Change activation function here\n",
    "model = CustomTransformerModel(input_dim, model_dim, num_heads, num_layers, ff_dim, n_classes, sequence_length, activation=activation_function).to(device)\n",
    "\n",
    "# Initialize weights\n",
    "def initialize_weights(m):\n",
    "    if isinstance(m, nn.Linear):\n",
    "        torch.nn.init.xavier_uniform_(m.weight)\n",
    "        m.bias.data.fill_(0.01)\n",
    "\n",
    "model.apply(initialize_weights)\n",
    "\n",
    "# Set optimizer with a lower learning rate\n",
    "optimizer = optim.AdamW(model.parameters(), lr=0.0001, weight_decay=1e-4)  # Reduced weight decay\n",
    "\n",
    "# Define loss function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Create dataset and dataloaders\n",
    "output_csv = 'D:\\\\Projects\\\\Resources\\\\midis_v1.2\\\\Beethoven\\\\Beethoven_note_sequence.csv'\n",
    "full_dataset = PianoDataset(output_csv, sequence_length, augment=True)\n",
    "\n",
    "# Split the dataset\n",
    "train_dataset, valid_dataset, test_dataset = train_valid_test_split(full_dataset)\n",
    "\n",
    "# Create DataLoaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=512, shuffle=True)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=512, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# Training function with gradient clipping and early stopping\n",
    "def train_model(model, train_loader, valid_loader, optimizer, num_epochs=100, patience=15):\n",
    "    model.train()\n",
    "    best_accuracy = 0.0\n",
    "    patience_counter = 0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss = 0.0\n",
    "        correct_predictions = 0\n",
    "        total_predictions = 0\n",
    "        print(f'Starting epoch {epoch+1}/{num_epochs}')\n",
    "        \n",
    "        for batch_idx, (inputs, targets) in enumerate(train_loader):\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            \n",
    "            # Ensure outputs and targets are correctly shaped\n",
    "            if outputs.shape[0] != targets.shape[0]:\n",
    "                print(f\"Shape mismatch: outputs shape {outputs.shape}, targets shape {targets.shape}\")\n",
    "                continue\n",
    "            \n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)  # Gradient clipping\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item() * inputs.size(0)  # Use inputs.size(0) for batch size\n",
    "            \n",
    "            # Calculate accuracy\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            correct_predictions += (predicted == targets).sum().item()\n",
    "            total_predictions += targets.size(0)\n",
    "\n",
    "        epoch_loss = running_loss / len(train_loader.dataset)\n",
    "        epoch_accuracy = correct_predictions / total_predictions\n",
    "        print(f'Epoch {epoch+1}/{num_epochs}, Loss: {epoch_loss:.4f}, Accuracy: {epoch_accuracy:.4f}')\n",
    "        \n",
    "        # Validate the model\n",
    "        val_loss, val_accuracy = evaluate_model(model, valid_loader)\n",
    "        \n",
    "        # Early stopping\n",
    "        if val_accuracy > best_accuracy:\n",
    "            best_accuracy = val_accuracy\n",
    "            patience_counter = 0\n",
    "            torch.save(model.state_dict(), 'best_model.pth')\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= patience:\n",
    "                print(f'Early stopping at epoch {epoch+1}')\n",
    "                break\n",
    "\n",
    "def evaluate_model(model, dataloader):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    correct_predictions = 0\n",
    "    total_predictions = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in dataloader:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            running_loss += loss.item() * inputs.size(0)  \n",
    "            \n",
    "            # Calculate accuracy\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            correct_predictions += (predicted == targets).sum().item()\n",
    "            total_predictions += targets.size(0)\n",
    "\n",
    "    total_loss = running_loss / len(dataloader.dataset)\n",
    "    accuracy = correct_predictions / total_predictions\n",
    "    print(f'Evaluation Loss: {total_loss:.4f}, Accuracy: {accuracy:.4f}')\n",
    "    return total_loss, accuracy\n",
    "\n",
    "# Train the model with high patience early stopping and save the best model\n",
    "train_model(model, train_loader, valid_loader, optimizer, num_epochs=100, patience=15)\n",
    "\n",
    "# Load the best model and evaluate on the test set\n",
    "model.load_state_dict(torch.load('best_model.pth'))\n",
    "evaluate_model(model, test_loader)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result remains similar to the original model."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
