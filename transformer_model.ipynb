{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first part would be load the MIDI file and convert it to sequence that coule be directly used in the model. Here we only trained with only 12 piano notes (Beethoven's piano solo) for shorter training process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from mido import MidiFile\n",
    "import csv\n",
    "\n",
    "def midi_to_note_sequence_to_file(filepath, output_csv):\n",
    "    with open(output_csv, 'w', newline='') as csvfile:\n",
    "        fieldnames = ['piece_id','event_type', 'control', 'value', 'note', 'velocity', 'time_since_last_event', 'channel']\n",
    "        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "        \n",
    "        writer.writeheader()\n",
    "        piece_id = 0\n",
    "\n",
    "        for filename in os.listdir(filepath):\n",
    "            if filename.endswith('.mid') or filename.endswith('.midi'):\n",
    "                try:\n",
    "                    mid = MidiFile(os.path.join(filepath, filename))  \n",
    "                    for track in mid.tracks:\n",
    "                        time_since_last_event = 0\n",
    "                        for msg in track:\n",
    "                            time_since_last_event += msg.time\n",
    "                            if not msg.is_meta:\n",
    "                                if msg.type in ['note_on', 'note_off']:\n",
    "                                    event_type = 'note_start' if msg.type == 'note_on' and msg.velocity > 0 else 'note_stop'\n",
    "                                    writer.writerow({\n",
    "                                        'piece_id': piece_id,\n",
    "                                        'event_type': event_type,\n",
    "                                        'control': 0,\n",
    "                                        'value': 0,\n",
    "                                        'note': msg.note,\n",
    "                                        'velocity': msg.velocity,\n",
    "                                        'time_since_last_event': time_since_last_event,\n",
    "                                        'channel': msg.channel\n",
    "                                    })\n",
    "                                elif msg.type == 'control_change':\n",
    "                                    writer.writerow({\n",
    "                                        'piece_id': piece_id,\n",
    "                                        'event_type': 'control_change',\n",
    "                                        'control': msg.control,\n",
    "                                        'value': msg.value,\n",
    "                                        'note': 0,\n",
    "                                        'velocity': 0,\n",
    "                                        'time_since_last_event': time_since_last_event,\n",
    "                                        'channel': msg.channel\n",
    "                                    })\n",
    "                                time_since_last_event = 0\n",
    "                    piece_id += 1\n",
    "                except OSError as e:\n",
    "                    print(f\"Error processing {filename}: {e}\")\n",
    "\n",
    "path = 'D:\\\\Projects\\\\Resources\\\\midis_v1.2\\\\Beethoven'\n",
    "output_csv = 'D:\\\\Projects\\\\Resources\\\\midis_v1.2\\\\Beethoven\\\\Beethoven_note_sequence.csv'\n",
    "midi_to_note_sequence_to_file(path, output_csv)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the main training and evaluating below, as we can see from the result, the accuracy stuck at around 0.64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting epoch 1/100\n",
      "Epoch 1/100, Loss: 1.1528, Accuracy: 0.5343\n",
      "Evaluation Loss: 1.0568, Accuracy: 0.5845\n",
      "Starting epoch 2/100\n",
      "Epoch 2/100, Loss: 0.9910, Accuracy: 0.6004\n",
      "Evaluation Loss: 0.9564, Accuracy: 0.6062\n",
      "Starting epoch 3/100\n",
      "Epoch 3/100, Loss: 0.9346, Accuracy: 0.6166\n",
      "Evaluation Loss: 0.9250, Accuracy: 0.6175\n",
      "Starting epoch 4/100\n",
      "Epoch 4/100, Loss: 0.9124, Accuracy: 0.6240\n",
      "Evaluation Loss: 0.9245, Accuracy: 0.6198\n",
      "Starting epoch 5/100\n",
      "Epoch 5/100, Loss: 0.8970, Accuracy: 0.6296\n",
      "Evaluation Loss: 0.8977, Accuracy: 0.6306\n",
      "Starting epoch 6/100\n",
      "Epoch 6/100, Loss: 0.8863, Accuracy: 0.6337\n",
      "Evaluation Loss: 0.8944, Accuracy: 0.6297\n",
      "Starting epoch 7/100\n",
      "Epoch 7/100, Loss: 0.8763, Accuracy: 0.6382\n",
      "Evaluation Loss: 0.8845, Accuracy: 0.6371\n",
      "Starting epoch 8/100\n",
      "Epoch 8/100, Loss: 0.8669, Accuracy: 0.6423\n",
      "Evaluation Loss: 0.8804, Accuracy: 0.6355\n",
      "Starting epoch 9/100\n",
      "Epoch 9/100, Loss: 0.8588, Accuracy: 0.6457\n",
      "Evaluation Loss: 0.8798, Accuracy: 0.6355\n",
      "Starting epoch 10/100\n",
      "Epoch 10/100, Loss: 0.8505, Accuracy: 0.6494\n",
      "Evaluation Loss: 0.8704, Accuracy: 0.6410\n",
      "Starting epoch 11/100\n",
      "Epoch 11/100, Loss: 0.8441, Accuracy: 0.6520\n",
      "Evaluation Loss: 0.8723, Accuracy: 0.6453\n",
      "Starting epoch 12/100\n",
      "Epoch 12/100, Loss: 0.8355, Accuracy: 0.6558\n",
      "Evaluation Loss: 0.8833, Accuracy: 0.6367\n",
      "Starting epoch 13/100\n",
      "Epoch 13/100, Loss: 0.8273, Accuracy: 0.6596\n",
      "Evaluation Loss: 0.8669, Accuracy: 0.6433\n",
      "Starting epoch 14/100\n",
      "Epoch 14/100, Loss: 0.8193, Accuracy: 0.6634\n",
      "Evaluation Loss: 0.8783, Accuracy: 0.6391\n",
      "Starting epoch 15/100\n",
      "Epoch 15/100, Loss: 0.8104, Accuracy: 0.6668\n",
      "Evaluation Loss: 0.8697, Accuracy: 0.6424\n",
      "Starting epoch 16/100\n",
      "Epoch 16/100, Loss: 0.8016, Accuracy: 0.6707\n",
      "Evaluation Loss: 0.8676, Accuracy: 0.6439\n",
      "Starting epoch 17/100\n",
      "Epoch 17/100, Loss: 0.7929, Accuracy: 0.6747\n",
      "Evaluation Loss: 0.8745, Accuracy: 0.6436\n",
      "Starting epoch 18/100\n",
      "Epoch 18/100, Loss: 0.7844, Accuracy: 0.6784\n",
      "Evaluation Loss: 0.8743, Accuracy: 0.6433\n",
      "Starting epoch 19/100\n",
      "Epoch 19/100, Loss: 0.7746, Accuracy: 0.6830\n",
      "Evaluation Loss: 0.8799, Accuracy: 0.6452\n",
      "Starting epoch 20/100\n",
      "Epoch 20/100, Loss: 0.7647, Accuracy: 0.6866\n",
      "Evaluation Loss: 0.8836, Accuracy: 0.6425\n",
      "Starting epoch 21/100\n",
      "Epoch 21/100, Loss: 0.7555, Accuracy: 0.6909\n",
      "Evaluation Loss: 0.8917, Accuracy: 0.6360\n",
      "Starting epoch 22/100\n",
      "Epoch 22/100, Loss: 0.7444, Accuracy: 0.6956\n",
      "Evaluation Loss: 0.8960, Accuracy: 0.6402\n",
      "Starting epoch 23/100\n",
      "Epoch 23/100, Loss: 0.7332, Accuracy: 0.7000\n",
      "Evaluation Loss: 0.8966, Accuracy: 0.6371\n",
      "Starting epoch 24/100\n",
      "Epoch 24/100, Loss: 0.7216, Accuracy: 0.7061\n",
      "Evaluation Loss: 0.9035, Accuracy: 0.6324\n",
      "Starting epoch 25/100\n",
      "Epoch 25/100, Loss: 0.7099, Accuracy: 0.7105\n",
      "Evaluation Loss: 0.9108, Accuracy: 0.6347\n",
      "Starting epoch 26/100\n",
      "Epoch 26/100, Loss: 0.6976, Accuracy: 0.7162\n",
      "Evaluation Loss: 0.9288, Accuracy: 0.6341\n",
      "Early stopping at epoch 26\n",
      "Evaluation Loss: 0.8742, Accuracy: 0.6438\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.8742001934206507, 0.6438128755934792)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import torch.optim as optim\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, model_dim, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        pe = torch.zeros(max_len, model_dim)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, model_dim, 2).float() * (-np.log(10000.0) / model_dim))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:, :x.size(1)]\n",
    "        return x\n",
    "\n",
    "class CustomTransformerEncoderLayer(nn.Module):\n",
    "    def __init__(self, model_dim, num_heads, ff_dim, dropout=0.1, activation='relu'):\n",
    "        super(CustomTransformerEncoderLayer, self).__init__()\n",
    "        self.self_attn = nn.MultiheadAttention(model_dim, num_heads, dropout=dropout, batch_first=True)\n",
    "        self.linear1 = nn.Linear(model_dim, ff_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.linear2 = nn.Linear(ff_dim, model_dim)\n",
    "        \n",
    "        # Choose activation function\n",
    "        if activation == 'relu':\n",
    "            self.activation = nn.ReLU()\n",
    "        elif activation == 'gelu':\n",
    "            self.activation = nn.GELU()\n",
    "        elif activation == 'softmax':\n",
    "            self.activation = nn.Softmax(dim=-1)\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported activation function: {activation}\")\n",
    "        \n",
    "        self.norm1 = nn.LayerNorm(model_dim)\n",
    "        self.norm2 = nn.LayerNorm(model_dim)\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, src):\n",
    "        src2 = self.self_attn(src, src, src)[0]\n",
    "        src = src + self.dropout1(src2)\n",
    "        src = self.norm1(src)\n",
    "        src2 = self.linear2(self.dropout(self.activation(self.linear1(src))))\n",
    "        src = src + self.dropout2(src2)\n",
    "        src = self.norm2(src)\n",
    "        return src\n",
    "\n",
    "class CustomTransformerModel(nn.Module):\n",
    "    def __init__(self, input_dim, model_dim, num_heads, num_layers, ff_dim, n_classes, dropout=0.1, activation='relu'):\n",
    "        super(CustomTransformerModel, self).__init__()\n",
    "        self.model_dim = model_dim\n",
    "        self.input_projection = nn.Linear(input_dim, model_dim)  \n",
    "        self.pos_encoder = PositionalEncoding(model_dim)\n",
    "        \n",
    "        # Create custom transformer encoder layers\n",
    "        self.encoder_layers = nn.ModuleList([\n",
    "            CustomTransformerEncoderLayer(model_dim, num_heads, ff_dim, dropout, activation)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "        \n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(in_features=model_dim * sequence_length, out_features=n_classes)\n",
    "        )\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        initrange = 0.1\n",
    "        for layer in self.classifier:\n",
    "            if isinstance(layer, nn.Linear):\n",
    "                layer.bias.data.zero_()\n",
    "                layer.weight.data.uniform_(-initrange, initrange)\n",
    "\n",
    "    def forward(self, src):\n",
    "        src = self.input_projection(src) * np.sqrt(self.model_dim)  # Project input and scale\n",
    "        src = self.pos_encoder(src)\n",
    "        for layer in self.encoder_layers:\n",
    "            src = layer(src)\n",
    "        output = self.classifier(src)\n",
    "        return output\n",
    "\n",
    "class PianoDataset(Dataset):\n",
    "    def __init__(self, csv_file, sequence_length=100):\n",
    "        self.df = pd.read_csv(csv_file)\n",
    "        self.df = pd.concat([self.df.drop('event_type', axis=1), pd.get_dummies(self.df['event_type'], prefix='event_type')], axis=1)\n",
    "        self.df = self.df.astype(np.float32)  \n",
    "        self.sequence_length = sequence_length\n",
    "        self.data = self.generate_sequences()\n",
    "\n",
    "    def generate_sequences(self):\n",
    "        input_sequences = []\n",
    "        target_events = []\n",
    "        for i in range(len(self.df) - self.sequence_length):\n",
    "            full_sequence = self.df.iloc[i:i+self.sequence_length+1]\n",
    "            input_sequence = full_sequence.iloc[:-1]\n",
    "            target_event = full_sequence.iloc[-1]\n",
    "            input_sequences.append(input_sequence.values)\n",
    "            target_events.append(target_event.values.argmax())  # Use class index as target\n",
    "        return list(zip(input_sequences, target_events))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sequence, target = self.data[idx]\n",
    "        sequence_tensor = torch.tensor(sequence, dtype=torch.float32)\n",
    "        target_tensor = torch.tensor(target, dtype=torch.long)\n",
    "        return sequence_tensor, target_tensor\n",
    "\n",
    "def train_valid_test_split(dataset, train_ratio=0.7, valid_ratio=0.15, test_ratio=0.15):\n",
    "    assert train_ratio + valid_ratio + test_ratio == 1, \"Ratios must sum to 1\"\n",
    "    \n",
    "    train_size = int(train_ratio * len(dataset))\n",
    "    valid_size = int(valid_ratio * len(dataset))\n",
    "    test_size = len(dataset) - train_size - valid_size\n",
    "    \n",
    "    train_dataset, valid_dataset, test_dataset = random_split(dataset, [train_size, valid_size, test_size])\n",
    "    return train_dataset, valid_dataset, test_dataset\n",
    "\n",
    "# Define model parameters\n",
    "input_dim = 10  \n",
    "model_dim = 128  \n",
    "num_heads = 4  \n",
    "num_layers = 8  \n",
    "ff_dim = 512  \n",
    "n_classes = 8  \n",
    "sequence_length = 100\n",
    "\n",
    "# Set device and initialize model and optimizer\n",
    "device = torch.device(\"cuda\")\n",
    "activation_function = 'gelu'\n",
    "model = CustomTransformerModel(input_dim, model_dim, num_heads, num_layers, ff_dim, n_classes, activation=activation_function).to(device)\n",
    "\n",
    "# Initialize weights\n",
    "def initialize_weights(m):\n",
    "    if isinstance(m, nn.Linear):\n",
    "        torch.nn.init.xavier_uniform_(m.weight)\n",
    "        m.bias.data.fill_(0.01)\n",
    "\n",
    "model.apply(initialize_weights)\n",
    "\n",
    "# Set optimizer with a lower learning rate\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
    "\n",
    "# Define loss function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Create dataset and dataloaders\n",
    "output_csv = 'D:\\\\Projects\\\\Resources\\\\midis_v1.2\\\\Beethoven\\\\Beethoven_note_sequence.csv'\n",
    "full_dataset = PianoDataset(output_csv, sequence_length)\n",
    "\n",
    "# Split the dataset\n",
    "train_dataset, valid_dataset, test_dataset = train_valid_test_split(full_dataset)\n",
    "\n",
    "# Create DataLoaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=512, shuffle=True)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=512, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# Training function with gradient clipping and early stopping\n",
    "def train_model(model, train_loader, valid_loader, optimizer, num_epochs=100, patience=5):\n",
    "    model.train()\n",
    "    best_accuracy = 0.0\n",
    "    patience_counter = 0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss = 0.0\n",
    "        correct_predictions = 0\n",
    "        total_predictions = 0\n",
    "        print(f'Starting epoch {epoch+1}/{num_epochs}')\n",
    "        \n",
    "        for batch_idx, (inputs, targets) in enumerate(train_loader):\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            \n",
    "            # Ensure outputs and targets are correctly shaped\n",
    "            if outputs.shape[0] != targets.shape[0]:\n",
    "                print(f\"Shape mismatch: outputs shape {outputs.shape}, targets shape {targets.shape}\")\n",
    "                continue\n",
    "            \n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item() * inputs.size(0)  \n",
    "            \n",
    "            # Calculate accuracy\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            correct_predictions += (predicted == targets).sum().item()\n",
    "            total_predictions += targets.size(0)\n",
    "\n",
    "        epoch_loss = running_loss / len(train_loader.dataset)\n",
    "        epoch_accuracy = correct_predictions / total_predictions\n",
    "        print(f'Epoch {epoch+1}/{num_epochs}, Loss: {epoch_loss:.4f}, Accuracy: {epoch_accuracy:.4f}')\n",
    "        \n",
    "        # Validate the model\n",
    "        val_loss, val_accuracy = evaluate_model(model, valid_loader)\n",
    "        \n",
    "        # Early stopping\n",
    "        if val_accuracy > best_accuracy:\n",
    "            best_accuracy = val_accuracy\n",
    "            patience_counter = 0\n",
    "            torch.save(model.state_dict(), 'best_model.pth')\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= patience:\n",
    "                print(f'Early stopping at epoch {epoch+1}')\n",
    "                break\n",
    "\n",
    "def evaluate_model(model, dataloader):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    correct_predictions = 0\n",
    "    total_predictions = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in dataloader:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            running_loss += loss.item() * inputs.size(0)  \n",
    "            \n",
    "            # Calculate accuracy\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            correct_predictions += (predicted == targets).sum().item()\n",
    "            total_predictions += targets.size(0)\n",
    "\n",
    "    total_loss = running_loss / len(dataloader.dataset)\n",
    "    accuracy = correct_predictions / total_predictions\n",
    "    print(f'Evaluation Loss: {total_loss:.4f}, Accuracy: {accuracy:.4f}')\n",
    "    return total_loss, accuracy\n",
    "\n",
    "# Train the model with high patience early stopping and save the best model\n",
    "train_model(model, train_loader, valid_loader, optimizer, num_epochs=100, patience=15)\n",
    "\n",
    "# Load the best model and evaluate on the test set\n",
    "model.load_state_dict(torch.load('best_model.pth'))\n",
    "evaluate_model(model, test_loader)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can try to modify the model with some approaches in cluding data augmenting and normalization and pretraining. But even with tuning, the accuracy are stuck. I would presume that large sample and longer-training with more complex structure transformer can achieve higher accuracy, but for the limited computational power, we might need to integrate some other models.p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting epoch 1/100\n",
      "Epoch 1/100, Loss: 1.1418, Accuracy: 0.5394\n",
      "Evaluation Loss: 1.0558, Accuracy: 0.5886\n",
      "Starting epoch 2/100\n",
      "Epoch 2/100, Loss: 0.9843, Accuracy: 0.6050\n",
      "Evaluation Loss: 0.9735, Accuracy: 0.6045\n",
      "Starting epoch 3/100\n",
      "Epoch 3/100, Loss: 0.9368, Accuracy: 0.6165\n",
      "Evaluation Loss: 0.9482, Accuracy: 0.6111\n",
      "Starting epoch 4/100\n",
      "Epoch 4/100, Loss: 0.9127, Accuracy: 0.6248\n",
      "Evaluation Loss: 0.9123, Accuracy: 0.6233\n",
      "Starting epoch 5/100\n",
      "Epoch 5/100, Loss: 0.8976, Accuracy: 0.6300\n",
      "Evaluation Loss: 0.9053, Accuracy: 0.6250\n",
      "Starting epoch 6/100\n",
      "Epoch 6/100, Loss: 0.8864, Accuracy: 0.6347\n",
      "Evaluation Loss: 0.8892, Accuracy: 0.6339\n",
      "Starting epoch 7/100\n",
      "Epoch 7/100, Loss: 0.8758, Accuracy: 0.6387\n",
      "Evaluation Loss: 0.8776, Accuracy: 0.6392\n",
      "Starting epoch 8/100\n",
      "Epoch 8/100, Loss: 0.8671, Accuracy: 0.6424\n",
      "Evaluation Loss: 0.8740, Accuracy: 0.6388\n",
      "Starting epoch 9/100\n",
      "Epoch 9/100, Loss: 0.8608, Accuracy: 0.6451\n",
      "Evaluation Loss: 0.8710, Accuracy: 0.6415\n",
      "Starting epoch 10/100\n",
      "Epoch 10/100, Loss: 0.8529, Accuracy: 0.6489\n",
      "Evaluation Loss: 0.8806, Accuracy: 0.6381\n",
      "Starting epoch 11/100\n",
      "Epoch 11/100, Loss: 0.8458, Accuracy: 0.6516\n",
      "Evaluation Loss: 0.8847, Accuracy: 0.6329\n",
      "Starting epoch 12/100\n",
      "Epoch 12/100, Loss: 0.8375, Accuracy: 0.6547\n",
      "Evaluation Loss: 0.8648, Accuracy: 0.6419\n",
      "Starting epoch 13/100\n",
      "Epoch 13/100, Loss: 0.8285, Accuracy: 0.6588\n",
      "Evaluation Loss: 0.8685, Accuracy: 0.6433\n",
      "Starting epoch 14/100\n",
      "Epoch 14/100, Loss: 0.8222, Accuracy: 0.6614\n",
      "Evaluation Loss: 0.8664, Accuracy: 0.6412\n",
      "Starting epoch 15/100\n",
      "Epoch 15/100, Loss: 0.8149, Accuracy: 0.6647\n",
      "Evaluation Loss: 0.8646, Accuracy: 0.6448\n",
      "Starting epoch 16/100\n",
      "Epoch 16/100, Loss: 0.8067, Accuracy: 0.6687\n",
      "Evaluation Loss: 0.8752, Accuracy: 0.6356\n",
      "Starting epoch 17/100\n",
      "Epoch 17/100, Loss: 0.8002, Accuracy: 0.6707\n",
      "Evaluation Loss: 0.8664, Accuracy: 0.6402\n",
      "Starting epoch 18/100\n",
      "Epoch 18/100, Loss: 0.7913, Accuracy: 0.6755\n",
      "Evaluation Loss: 0.8754, Accuracy: 0.6423\n",
      "Starting epoch 19/100\n",
      "Epoch 19/100, Loss: 0.7820, Accuracy: 0.6788\n",
      "Evaluation Loss: 0.8703, Accuracy: 0.6447\n",
      "Starting epoch 20/100\n",
      "Epoch 20/100, Loss: 0.7728, Accuracy: 0.6831\n",
      "Evaluation Loss: 0.8749, Accuracy: 0.6429\n",
      "Starting epoch 21/100\n",
      "Epoch 21/100, Loss: 0.7635, Accuracy: 0.6876\n",
      "Evaluation Loss: 0.8808, Accuracy: 0.6438\n",
      "Starting epoch 22/100\n",
      "Epoch 22/100, Loss: 0.7532, Accuracy: 0.6923\n",
      "Evaluation Loss: 0.8832, Accuracy: 0.6424\n",
      "Starting epoch 23/100\n",
      "Epoch 23/100, Loss: 0.7430, Accuracy: 0.6963\n",
      "Evaluation Loss: 0.8897, Accuracy: 0.6396\n",
      "Starting epoch 24/100\n",
      "Epoch 24/100, Loss: 0.7335, Accuracy: 0.7007\n",
      "Evaluation Loss: 0.8983, Accuracy: 0.6414\n",
      "Starting epoch 25/100\n",
      "Epoch 25/100, Loss: 0.7233, Accuracy: 0.7049\n",
      "Evaluation Loss: 0.8954, Accuracy: 0.6356\n",
      "Starting epoch 26/100\n",
      "Epoch 26/100, Loss: 0.7178, Accuracy: 0.7075\n",
      "Evaluation Loss: 0.9042, Accuracy: 0.6350\n",
      "Starting epoch 27/100\n",
      "Epoch 27/100, Loss: 0.7050, Accuracy: 0.7130\n",
      "Evaluation Loss: 0.9090, Accuracy: 0.6341\n",
      "Starting epoch 28/100\n",
      "Epoch 28/100, Loss: 0.6924, Accuracy: 0.7185\n",
      "Evaluation Loss: 0.9145, Accuracy: 0.6357\n",
      "Starting epoch 29/100\n",
      "Epoch 29/100, Loss: 0.6793, Accuracy: 0.7243\n",
      "Evaluation Loss: 0.9296, Accuracy: 0.6319\n",
      "Starting epoch 30/100\n",
      "Epoch 30/100, Loss: 0.6686, Accuracy: 0.7288\n",
      "Evaluation Loss: 0.9353, Accuracy: 0.6303\n",
      "Early stopping at epoch 30\n",
      "Evaluation Loss: 0.8673, Accuracy: 0.6426\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.867312515857325, 0.6425512135197051)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import torch.optim as optim\n",
    "import random\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, model_dim, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        pe = torch.zeros(max_len, model_dim)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, model_dim, 2).float() * (-np.log(10000.0) / model_dim))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:, :x.size(1)]\n",
    "        return x\n",
    "\n",
    "class CustomTransformerEncoderLayer(nn.Module):\n",
    "    def __init__(self, model_dim, num_heads, ff_dim, dropout=0.1, activation='relu'):\n",
    "        super(CustomTransformerEncoderLayer, self).__init__()\n",
    "        self.self_attn = nn.MultiheadAttention(model_dim, num_heads, dropout=dropout, batch_first=True)\n",
    "        self.linear1 = nn.Linear(model_dim, ff_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.linear2 = nn.Linear(ff_dim, model_dim)\n",
    "        \n",
    "        # Choose activation function\n",
    "        if activation == 'relu':\n",
    "            self.activation = nn.ReLU()\n",
    "        elif activation == 'gelu':\n",
    "            self.activation = nn.GELU()\n",
    "        elif activation == 'softmax':\n",
    "            self.activation = nn.Softmax(dim=-1)\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported activation function: {activation}\")\n",
    "        \n",
    "        self.norm1 = nn.LayerNorm(model_dim)\n",
    "        self.norm2 = nn.LayerNorm(model_dim)\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, src):\n",
    "        src2 = self.self_attn(src, src, src)[0]\n",
    "        src = src + self.dropout1(src2)\n",
    "        src = self.norm1(src)\n",
    "        src2 = self.linear2(self.dropout(self.activation(self.linear1(src))))\n",
    "        src = src + self.dropout2(src2)\n",
    "        src = self.norm2(src)\n",
    "        return src\n",
    "\n",
    "class CustomTransformerModel(nn.Module):\n",
    "    def __init__(self, input_dim, model_dim, num_heads, num_layers, ff_dim, n_classes, dropout=0.1, activation='relu'):\n",
    "        super(CustomTransformerModel, self).__init__()\n",
    "        self.model_dim = model_dim\n",
    "        self.input_projection = nn.Linear(input_dim, model_dim)  # Project input to model dimension\n",
    "        self.pos_encoder = PositionalEncoding(model_dim)\n",
    "        \n",
    "        # Create custom transformer encoder layers\n",
    "        self.encoder_layers = nn.ModuleList([\n",
    "            CustomTransformerEncoderLayer(model_dim, num_heads, ff_dim, dropout, activation)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "        \n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(in_features=model_dim * sequence_length, out_features=n_classes)\n",
    "        )\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        initrange = 0.1\n",
    "        for layer in self.classifier:\n",
    "            if isinstance(layer, nn.Linear):\n",
    "                layer.bias.data.zero_()\n",
    "                layer.weight.data.uniform_(-initrange, initrange)\n",
    "\n",
    "    def forward(self, src):\n",
    "        src = self.input_projection(src) * np.sqrt(self.model_dim)  # Project input and scale\n",
    "        src = self.pos_encoder(src)\n",
    "        for layer in self.encoder_layers:\n",
    "            src = layer(src)\n",
    "        output = self.classifier(src)\n",
    "        return output\n",
    "\n",
    "class PianoDataset(Dataset):\n",
    "    def __init__(self, csv_file, sequence_length=100, augment=False):\n",
    "        self.df = pd.read_csv(csv_file)\n",
    "        self.df = pd.concat([self.df.drop('event_type', axis=1), pd.get_dummies(self.df['event_type'], prefix='event_type')], axis=1)\n",
    "        self.df = self.df.astype(np.float32)  # Ensure all data is of type float32\n",
    "        self.sequence_length = sequence_length\n",
    "        self.augment = augment\n",
    "        self.data = self.generate_sequences()\n",
    "\n",
    "    def generate_sequences(self):\n",
    "        input_sequences = []\n",
    "        target_events = []\n",
    "        for i in range(len(self.df) - self.sequence_length):\n",
    "            full_sequence = self.df.iloc[i:i+self.sequence_length+1]\n",
    "            input_sequence = full_sequence.iloc[:-1]\n",
    "            target_event = full_sequence.iloc[-1]\n",
    "            input_sequences.append(input_sequence.values)\n",
    "            target_events.append(target_event.values.argmax())  # Use class index as target\n",
    "        return list(zip(input_sequences, target_events))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sequence, target = self.data[idx]\n",
    "        if self.augment:\n",
    "            sequence = self.augment_sequence(sequence)\n",
    "        sequence_tensor = torch.tensor(sequence, dtype=torch.float32)\n",
    "        target_tensor = torch.tensor(target, dtype=torch.long)\n",
    "        return sequence_tensor, target_tensor\n",
    "\n",
    "    def augment_sequence(self, sequence):\n",
    "        # Transposition\n",
    "        if random.random() < 0.5:\n",
    "            transpose_amount = random.randint(-5, 5)\n",
    "            sequence[:, 0] += transpose_amount  \n",
    "        \n",
    "        # Time-stretching\n",
    "        if random.random() < 0.5:\n",
    "            stretch_factor = random.uniform(0.8, 1.2)\n",
    "            sequence[:, 1] *= stretch_factor  \n",
    "        \n",
    "        # Adding Noise\n",
    "        if random.random() < 0.5:\n",
    "            noise = np.random.normal(0, 0.01, sequence.shape)\n",
    "            sequence += noise\n",
    "        \n",
    "        return sequence\n",
    "\n",
    "def train_valid_test_split(dataset, train_ratio=0.7, valid_ratio=0.15, test_ratio=0.15):\n",
    "    assert train_ratio + valid_ratio + test_ratio == 1, \"Ratios must sum to 1\"\n",
    "    \n",
    "    train_size = int(train_ratio * len(dataset))\n",
    "    valid_size = int(valid_ratio * len(dataset))\n",
    "    test_size = len(dataset) - train_size - valid_size\n",
    "    \n",
    "    train_dataset, valid_dataset, test_dataset = random_split(dataset, [train_size, valid_size, test_size])\n",
    "    return train_dataset, valid_dataset, test_dataset\n",
    "\n",
    "# Define model parameters\n",
    "input_dim = 10  # Number of features in the input\n",
    "model_dim = 128  # Model dimension\n",
    "num_heads = 4  # Number of heads\n",
    "num_layers = 8  # Increased number of layers\n",
    "ff_dim = 512  # Feedforward dimension\n",
    "n_classes = 8  # Number of classes for classification\n",
    "sequence_length = 100\n",
    "\n",
    "# Set device and initialize model and optimizer\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "activation_function = 'gelu'  # Change activation function here\n",
    "model = CustomTransformerModel(input_dim, model_dim, num_heads, num_layers, ff_dim, n_classes, activation=activation_function).to(device)\n",
    "\n",
    "# Initialize weights\n",
    "def initialize_weights(m):\n",
    "    if isinstance(m, nn.Linear):\n",
    "        torch.nn.init.xavier_uniform_(m.weight)\n",
    "        m.bias.data.fill_(0.01)\n",
    "\n",
    "model.apply(initialize_weights)\n",
    "\n",
    "# Set optimizer with a lower learning rate\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
    "\n",
    "# Define loss function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Create dataset and dataloaders\n",
    "output_csv = 'D:\\\\Projects\\\\Resources\\\\midis_v1.2\\\\Beethoven\\\\Beethoven_note_sequence.csv'\n",
    "full_dataset = PianoDataset(output_csv, sequence_length, augment=True)\n",
    "\n",
    "# Split the dataset\n",
    "train_dataset, valid_dataset, test_dataset = train_valid_test_split(full_dataset)\n",
    "\n",
    "# Create DataLoaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=512, shuffle=True)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=512, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# Training function with gradient clipping and early stopping\n",
    "def train_model(model, train_loader, valid_loader, optimizer, num_epochs=100, patience=5):\n",
    "    model.train()\n",
    "    best_accuracy = 0.0\n",
    "    patience_counter = 0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss = 0.0\n",
    "        correct_predictions = 0\n",
    "        total_predictions = 0\n",
    "        print(f'Starting epoch {epoch+1}/{num_epochs}')\n",
    "        \n",
    "        for batch_idx, (inputs, targets) in enumerate(train_loader):\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            \n",
    "            # Ensure outputs and targets are correctly shaped\n",
    "            if outputs.shape[0] != targets.shape[0]:\n",
    "                print(f\"Shape mismatch: outputs shape {outputs.shape}, targets shape {targets.shape}\")\n",
    "                continue\n",
    "            \n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item() * inputs.size(0)  # Use inputs.size(0) for batch size\n",
    "            \n",
    "            # Calculate accuracy\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            correct_predictions += (predicted == targets).sum().item()\n",
    "            total_predictions += targets.size(0)\n",
    "\n",
    "        epoch_loss = running_loss / len(train_loader.dataset)\n",
    "        epoch_accuracy = correct_predictions / total_predictions\n",
    "        print(f'Epoch {epoch+1}/{num_epochs}, Loss: {epoch_loss:.4f}, Accuracy: {epoch_accuracy:.4f}')\n",
    "        \n",
    "        # Validate the model\n",
    "        val_loss, val_accuracy = evaluate_model(model, valid_loader)\n",
    "        \n",
    "        # Early stopping\n",
    "        if val_accuracy > best_accuracy:\n",
    "            best_accuracy = val_accuracy\n",
    "            patience_counter = 0\n",
    "            torch.save(model.state_dict(), 'best_model.pth')\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= patience:\n",
    "                print(f'Early stopping at epoch {epoch+1}')\n",
    "                break\n",
    "\n",
    "def evaluate_model(model, dataloader):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    correct_predictions = 0\n",
    "    total_predictions = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in dataloader:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            running_loss += loss.item() * inputs.size(0)  \n",
    "            \n",
    "            # Calculate accuracy\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            correct_predictions += (predicted == targets).sum().item()\n",
    "            total_predictions += targets.size(0)\n",
    "\n",
    "    total_loss = running_loss / len(dataloader.dataset)\n",
    "    accuracy = correct_predictions / total_predictions\n",
    "    print(f'Evaluation Loss: {total_loss:.4f}, Accuracy: {accuracy:.4f}')\n",
    "    return total_loss, accuracy\n",
    "\n",
    "# Train the model with high patience early stopping and save the best model\n",
    "train_model(model, train_loader, valid_loader, optimizer, num_epochs=100, patience=15)\n",
    "\n",
    "# Load the best model and evaluate on the test set\n",
    "model.load_state_dict(torch.load('best_model.pth'))\n",
    "evaluate_model(model, test_loader)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result remains similar to the original model."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
